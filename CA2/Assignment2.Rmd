---
title: "Assignment II"
subtitle: "Logistic regression"
author: "Silpa Soni Nallacheruvu (19980824-5287) Hernan Aldana (20000526-4999)"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
```

---

# Exercise 2:1

This report analyses the data on periodontitis from a group of adult patients at a large dental clinic.
The goal is to understand the influence of the parameter estimates in logistic regression models of the probability for periodontitis on the population as a function of dental floss use and the probability for using dental floss as a function of periodontitis status. 

```{r echo=FALSE, fig.align='center'}

# Create the 2x2 table
periodontitis_table <- matrix(
  c(22, 75, 148, 265, 97, 413, 170, 340, 510), # Data including margins
  nrow = 3,                                   # Three rows: Yes, No, Sum
  byrow = TRUE,                               # Fill matrix by rows
  dimnames = list(
    `Dental Floss Use` = c("Used Dental Floss", "Not Used Dental Floss", "Total"), # Row names
    `Periodontitis` = c("Periodontitis", "No Periodontitis", "Total")    # Column names
  )
)

# Convert to data frame for better printing
periodontitis_df <- as.data.frame.matrix(periodontitis_table)

# Use kable to display the table
kable(
  periodontitis_df,
  caption = "Regular Use of Dental Floss and Periodontitis",
  align = "c"
)

```

## Question 1: Logistic regression model of the probability for periodontitis

The logistic regression model,say Model A of the probability for periodontitis in the population as a function of dental floss use is defined as:

$\text{Model A} : \text{logit}(p_x) = \log\left(\frac{p_x}{1 - p_x}\right) = \beta_0 + \beta_1 x$

where:

- $x$ = 1 if dental floss is regularly used, $x$ = 0 otherwise.
- $p_x$ = $P(\text{periodontitis} \mid x)$, the probability of periodontitis given $x$.
- $\beta_0$: The log-odds of periodontitis when $x$ = 0 (no floss use).
- $\beta_1$: The change in log-odds of periodontitis when $x$ changes from 0 to 1 (effect of using floss).
	
### Approach:

Fit the logistic regression model A to the data and interpret the parameter estimates $\beta_0$ and $\beta_1$.

### Estimate Parameters:

**Fit the logistic regression model:**

The summary of the coefficients of the logistic regression model A is presented below:

```{r echo=FALSE}
# Prepare data
dental_data <- data.frame(
  floss = c(1, 0), # 1 = yes, 0 = no
  periodontitis_yes = c(22, 148),
  periodontitis_no = c(75, 265)
)

# Logistic regression
model_floss <- glm(cbind(periodontitis_yes, periodontitis_no) ~ floss, 
             family = binomial(link=logit), data = dental_data)

# Extract model coefficients summary
model_floss_summary <- summary(model_floss)

# Create a data frame with relevant data
floss_coefficients_table <- data.frame(
  Term = rownames(model_floss_summary$coefficients),
  Estimate = model_floss_summary$coefficients[, "Estimate"],
  `Std. Error` = model_floss_summary$coefficients[, "Std. Error"],
  `z value` = model_floss_summary$coefficients[, "z value"],
  `Pr(>|z|)` = model_floss_summary$coefficients[, "Pr(>|z|)"]
)

# Print the table using kable
kable(
  floss_coefficients_table,
  caption = "Summary of Logistic Regression Model A Coefficients",
  align = c("l", "c", "c", "c", "c"),
  col.names = c("Term", "Estimate", "Std. Error", "z value", "P-value")
)

#print the AIC score
model_floss_aic <- AIC(model_floss)
print(paste("AIC of Model A:", model_floss_aic))
```

The final logistic regression equation: $\text{logit}(p_x) = -0.582 - 0.644x$

### Interpret the Estimates:

- 1.	$\beta_0$ = -0.582:
  - The log-odds of periodontitis for individuals who do not use dental floss is approximately -0.582.
  - The corresponding probability of periodontitis is: 
  $p_0 = \frac{e^{-0.582}}{1 + e^{-0.582}} \approx 0.358$
  
Individuals who do not use dental floss have a probability of approximately 35.8% of developing periodontitis compared to those who do.

- 2.	$\beta_1$ = -0.644:
  - The log-odds of periodontitis decreases by 0.644 when individuals use dental floss regularly.
  - This corresponds to an odds ratio of:
  $\text{Odds Ratio} = e^{\beta_1} = e^{-0.644} \approx 0.525$

Individuals who use dental floss regularly have approximately 52.5% lower odds of developing periodontitis compared to those who do not.

### Conclusion: 

- Regular dental floss use is associated with a significant reduction in the odds of periodontitis.
- The logistic regression model A is accurate because the binary data (periodontitis: yes/no) and the categorical explanatory variable (dental floss use) provide estimates that can be used to interpret the effect of dental floss use on periodontitis.

## Question 2: Logistic regression model of the probability for using dental floss

The logistic regression model, say Model B, of the probability for using dental floss as a function of periodontitis status is defined as:

$\text{logit}(p_y) = \log\left(\frac{p_y}{1 - p_y}\right) = \gamma_0 + \gamma_1 y$

where:

- $y$ = 1 if periodontitis is present, $y$ = 0 otherwise.
- $p_y$ = $P(\text{using dental floss} \mid y)$, the probability of using dental floss given $y$.
- $\gamma_0$: The log-odds of using dental floss when  $y$ = 0  (i.e., when periodontitis is absent).
- $\gamma_1$: The change in log-odds of using dental floss associated with the presence of periodontitis ( $y$ = 1 ).
	
### Approach:

Fit the logistic regression model B to the data and interpret the parameter estimates $\gamma_0$ and $\gamma_1$

### Estimate Parameters:

**Fit the logistic regression model:**

The summary of the coefficients of the logistic regression model B is presented below:

```{r echo=FALSE}
# Prepare data
periodontitis_data <- data.frame(
  periodontitis = c(1, 0), # 1 = yes, 0 = no
  floss_yes = c(22, 75),
  floss_no = c(148, 265)
)

# Logistic regression
model_periodontitis <- glm(cbind(floss_yes, floss_no) ~ periodontitis, 
             family = binomial(link=logit), data = periodontitis_data)

# Extract model coefficients summary
model_periodontitis_summary <- summary(model_periodontitis)

# Create a data frame with relevant data
coefficients_table2 <- data.frame(
  Term = rownames(model_periodontitis_summary$coefficients),
  Estimate = model_periodontitis_summary$coefficients[, "Estimate"],
  `Std. Error` = model_periodontitis_summary$coefficients[, "Std. Error"],
  `z value` = model_periodontitis_summary$coefficients[, "z value"],
  `Pr(>|z|)` = model_periodontitis_summary$coefficients[, "Pr(>|z|)"]
)

# Print the table using kable
kable(
  coefficients_table2,
  caption = "Summary of Logistic Regression Model B Coefficients",
  align = c("l", "c", "c", "c", "c", "c"),
  col.names = c("Term", "Estimate", "Std. Error", "z value", "P-value")
)

#print the AIC score
model_aic <- AIC(model_periodontitis)
print(paste("AIC of Model B:", model_aic))
```

The final logistic regression equation: $\text{logit}(p_y) = -1.262 - 0.644y$

### Interpret the Estimates:

- 1.	$\gamma_0$ = -1.262:
  - The log-odds of using dental floss when periodontitis is absent is approximately -1.262
  - The corresponding probability of using dental floss is: 
  $p_0 = \frac{e^{-1.262}}{1 + e^{-1.262}} \approx 0.2205$

Individuals without periodontitis have a probability of approximately 22.05% of using dental floss compared to those who do not.

- 2.	$\gamma_1$ = -0.644:
  - The log-odds of using dental floss decreases by 0.644 when periodontitis is present.
  - This corresponds to an odds ratio of:
  $\text{Odds Ratio} = e^{\gamma_1} = e^{-0.644} \approx 0.525$

The odds of individuals with periodontitis using dental floss are approximately 52.4% lower compared to not using dental floss.

### Conclusion: 

- The presence of periodontitis is associated with a significant reduction in the odds of regular dental floss use.
- The logistic regression model B is accurate because the binary data (Dental Floss use: yes/no) and the categorical explanatory variable (presence of periodontitis) provide estimates that can be used to interpret the presence of periodontitis on regular dental floss use. 


## Question 3: 

Compare the estimates of the parameters of Model A and Model B with the values derived using the definitive formulas of the log odds and probabilities.

### Approach : 

**1. Expected Estimates of the parameters of Model A:**

1.1.	Intercept $(\beta_0)$:

- $\beta_0$ represents the log-odds of periodontitis when $x$ = 0 (no dental floss use):

    $\beta_0 = \log\left(\frac{p_0}{1 - p_0}\right)$ where:
    
    $p_0 = P(\text{periodontitis} \mid x=0) = \frac{\text{Periodontitis (No Floss)}}{\text{Total (No Floss)}} = \frac{148}{413} \approx 0.3585$

- Hence: $\beta_0 \approx \log\left(\frac{0.3585}{1 - 0.3585}\right) = \log(0.558) \approx -0.582$

1.2.	Slope $(\beta_1)$:

- $\beta_1$ represents the change in log-odds when $x$ = 1 (dental floss is used):

    $\beta_1 = \log\left(\frac{p_1}{1 - p_1}\right) - \log\left(\frac{p_0}{1 - p_0}\right)$ where:
        
    $p_1 = P(\text{periodontitis} \mid x=1) = \frac{\text{Periodontitis (Floss)}}{\text{Total (Floss)}} = \frac{22}{97} \approx 0.2268$

- Hence:
      $\beta_1 \approx \log\left(\frac{0.2268}{1 - 0.2268}\right) - \log\left(\frac{0.3585}{1 - 0.3585}\right) \approx \log(0.293) - \log(0.558) = -1.226 + 0.582 = -0.644$


**2. Expected Estimates of the parameters of Model B:**

2.1.	Intercept $(\gamma_0)$:

- $\gamma_0$ represents the log-odds of using dental floss when $y$ = 0 (periodontitis is absent):

    $\gamma_0 = \log\left(\frac{p_0}{1 - p_0}\right)$ where:
    
    $p_0 = P(\text{using dental floss} \mid y=0)  = \frac{\text{Using Floss (No Periodontitis)}}{\text{Total (No Periodontitis)}} = \frac{75}{340} \approx 0.2205$

- Hence: $\gamma_0 \approx \log\left(\frac{0.2205}{1 - 0.2205}\right) = \log(0.282) \approx -1.262$ 


2.2.	Slope $(\gamma_1)$:

- $\gamma_1$ represents the change in log-odds of using dental floss when $y$ = 1 (periodontitis is present):

    $\gamma_1 = \log\left(\frac{p_1}{1 - p_1}\right) - \log\left(\frac{p_0}{1 - p_0}\right)$ where:
        
    $p_1 = P(\text{using dental floss} \mid y=1) = \frac{\text{Using Floss (Periodontitis)}}{\text{Total (Periodontitis)}} = \frac{22}{170} \approx 0.1294$

- Hence:
      $\gamma_1 \approx \log\left(\frac{0.1294}{1 - 0.1294}\right) - \log\left(\frac{0.2205}{1 - 0.2205}\right) \approx \log(0.149) - \log(0.282) = -1.906 + 1.262 = -0.644$


## Observation : 

- The expected estimates of the parameters of Model A and Model B are consistent with the actual estimates obtained from the logistic regression models.
- The slopes of the two logistic regression models $\beta_1$ and $\gamma_1$ are the same, indicating that the change in log-odds of periodontitis and dental floss use are consistent with each other.

---

\newpage

# Exercise 2:2
## Question 1:

For this question we estimate for each dose separately the risk, odds, and log-odds of developing a tumor. Also plot the risk (probability) of developing a tumor as a function of the log dose. Calculate the log-odds of developing a tumor and plot it as a function of the log dose. 

```{r echo=FALSE}
# Data from the instructions
log_dose <- c(-7.60, -6.22, -4.60, -3.00, -1.39, 0.92)
tumor <- c(1, 2, 4, 9, 12, 32)
no_tumor <- c(17, 17, 24, 23, 16, 8)
total <- tumor + no_tumor

# Create the table
original_table <- data.frame(
  `log(dose)` = log_dose,
  Tumor = tumor,
  `No tumor` = no_tumor,
  `Total` = total
)
t_table<- t(original_table)
# Display the table
knitr::kable(t_table, caption = "Original Data Table")
```

### Approach : 
To answer these questions we have to calculate the following estimates for each dose level of the table:

1. The risk of developing a tumor is calculated as:

  $Risk = \frac{Number of Tumor}{Total Observations}$

2. The odds of developing a tumor is calculated as:

  $Odds = \frac{Risk}{1 - Risk}$

3. The log-odds of developing a tumor is calculated as:

  $\log(Odds) = \log(\frac{Risk}{1-Risk})$

After we caclulate these estimates for each dose level, we  plot the risk (probability) of developing a tumor as a function of the log dose. We  also calculate the log-odds of developing a tumor and plot it as a function of the log dose.

### Results

```{r echo=FALSE}
# Calculate risk (probability) of tumor
risk <- tumor / total

# Calculate odds
odds <- risk / (1 - risk)

# Calculate log-odds
log_odds <- log(odds)

updated_table <- data.frame(
  `log(dose)` = log_dose,
  Tumor = tumor,
  `No tumor` = no_tumor,
  Total = total,
  Risk = round(risk, 4),
  Odds = round(odds, 4),
  `Log-Odds` = round(log_odds, 4)
)
t_updated_table<- t(updated_table)
knitr::kable(t_updated_table, caption = "Original Data Table with Risk, Odds, and Log-Odds Calculations")

```
1. Data

  - As we can observe from the updated table and from the given data, at lower doses of BaP, few mice develop tumors, while most do not. As the dose increases, the risk of developing a tumor also increases.

2. Risk

  - Risk represents the probability of developing a tumor, and for the given data it increases with $\log(dose)$. As for the calculated estimates, the risk is very low(0.0556) for the lowest dose of BaP(-7.60), indicating a small proportion of tumors.  At the highest dose (0.92) the risk is much higher(0.8), showing a significant increase in the development of tumors. 

3. Odds

  - Odds, which represent the ratio of tumor probability to no-tumor probability, increases expontentially with dose. The odds are very low(0.0588) for the lowest dose of BaP(-7.60), indicating a very low probability of developing a tumor. This means that tumors are 0.0588 as likely as no tumors. At the highest dose (0.92) the odds are much higher(4), showing a significant increase in the development of tumors. Meaning that tumors are four times more likely than no tumors at the given dose.
  
4. Log-Odds

  - Log-Odds increase nearly linearly with log(dose), ranging from -2.8332 at the lowest dose to 1.3863 at the highest. The linearity in log-odds is imperative, as it supports the use of a logistic regression.
  


```{r echo=FALSE, fig.align='center'}

# Create an updated table with the new calculations
updated_table <- data.frame(
  `log(dose)` = log_dose,
  Tumor = tumor,
  `No tumor` = no_tumor,
  Total = total,
  Risk = round(risk, 4),
  Odds = round(odds, 4),
  `Log-Odds` = round(log_odds, 4)
)

# Plot Risk (Probability) vs log(dose)
plot(log_dose, risk, type = "b", col = "blue", pch = 19,
     xlab = "log(Dose)", ylab = "Risk (Probability)",
     main = "Risk vs log(Dose)")
grid()
```

**1. Risk vs log(dose):**

  - The graph confirms a strong dose response relationship. As log(dose) increases, the probability of developing a tumor increases rapidly.
  


```{r echo=FALSE, fig.align='center'}
# Plot Log-Odds vs log(dose)
plot(log_dose, log_odds, type = "b", col = "red", pch = 19,
     xlab = "log(Dose)", ylab = "Log-Odds",
     main = "Log-Odds vs log(Dose)")
grid()
```

**2. Log-Odds vs log(dose):**

  - The graph shows a nearly linear relationship with log(dose) confirming that the data aligns well with the assumptions of logistic regression. 
  
### Conclusion

The analysis confirms a strong dose response relationship between the dose of BaP and the probability of developing a tumor. As the dose or log(dose) increases, the risk and odds of tumor development also increase as shown in the "Risk vs. log(dose)" plot. The near linear relationship in the "Log-Odds vs. log(dose)" plot supports the use of logistic regression to model the data.


## Question 2

Fit a logistic regression model to the data and interpret the parameter estimates, particularly the slope parameter.

### Approach :

To answer this question, we fit a logistic regression model where the probability of developing a tumor($P$) is modeled as:

$\text{logit}(P) = \log\left(\frac{P}{1 - P}\right) = \beta_0 + \beta_1 \times \text{log(dose})$

where:
  
  - $\beta_{0}$ is the intercept, which represents the log-odds of developing a tumor when the log(dose) is zero.
  
  - $\beta_{1}$ is the slope parameter, which represents the change in log-odds of developing a tumor for a one-unit increase in the log(dose).

We will use the logistic regression model to estimate this parameters.

### Results :

```{r echo=FALSE}
# Fit a logistic regression model
model_tumor <- glm(cbind(tumor, no_tumor) ~ log_dose, family = binomial)

# Summary of the model
summary_model_tumor <- summary(model_tumor)

# Create a data frame with relevant data
coefficients_table3 <- data.frame(
  Term = rownames(summary_model_tumor$coefficients),
  Estimate = summary_model_tumor$coefficients[, "Estimate"],
  `Std. Error` = summary_model_tumor$coefficients[, "Std. Error"],
  `z value` = summary_model_tumor$coefficients[, "z value"],
  `Pr(>|z|)` = summary_model_tumor$coefficients[, "Pr(>|z|)"]
)

# print the coefficients table
kable(
  coefficients_table3,
  caption = "Summary of Logistic Regression Model Coefficients",
  align = c("l", "c", "c", "c", "c"),
  col.names = c("Term", "Estimate", "Std. Error", "z value", "P-value")
)

# print the AIC score
tumor_model_aic <- AIC(model_tumor)
print(paste("AIC of the fitted model:", tumor_model_aic))

```



```{r echo=FALSE, fig.align='center'}
# Generate fitted values for plotting
new_dose <- seq(min(log_dose), max(log_dose), length.out = 100)
predicted_risk <- predict(model_tumor, newdata = data.frame(log_dose = new_dose), type = "response")

# Add fitted curve to the risk plot
plot(log_dose, risk, type = "b", col = "blue", pch = 19,
     xlab = "log(Dose)", ylab = "Risk (Probability)",
     main = "Risk vs log(Dose) with Logistic Fit")
lines(new_dose, predicted_risk, col = "darkgreen", lwd = 2, lty = 2)
legend("topleft", legend = c("Observed Risk", "Fitted Logistic Curve"),
       col = c("blue", "darkgreen"), lty = c(1, 2), lwd = c(1, 2), pch = c(19, NA))
grid()
```


**1. Intercept $\beta_0$:**

According to the summary of the model, the estimate of the intercept $\beta_{0}$ is 0.687. This represent the log-odds of developing a tumor when the log(dose) is zero. Its corresponding odd are $e^{0.687} = 1.99$. This means that the odds of developing a tumor are nearly twice as high as the odds of not developing a tumor when the log(dose) is zero.

Translating this to probabilities, the probability of developing a tumor is $P = \frac{e^{0.687}}{1 + e^{0.687}} = 0.665$. This means that the probability of developing a tumor is 66.5% when the log(dose) is zero.

**2. Slope $\beta_1$:**

The estimate of the slope parameter $\beta_{1}$ is 0.52037. This quantifies the change in the log-odds of developing a tumor for a one-unit increase in log(dose). A positive slope indicates that higher doses increase the odds of developing a tumor. the odds ratio is $e^{0.52037}\approx1.683$

This means that for each one-unit increase in log(dose), the odds of developing a tumor increases by approximately 68.3%


### Conclusion :

In conclusion, the logistic regression model reveals a clear relationship between log(dose) and the probability of developing a tumor. The intercept $\beta_{0}$ represents the log-odds of developing a tumor when the log(dose) is zero, while the slope $\beta_{1}$ quantifies the change in log-odds for a one-unit increase in log(dose). The positive slope indicates that higher doses increase the odds of developing a tumor. The fitted logistic curve aligns well with the observed risk, confirming the appropriateness of the logistic regression model.

## Question 3

Find the covariance matrix for the estimates and assess if they are correlated. Find a 95% confidence interval for the parameters. Find a 95% confidence interval for the tumor risk at dose $0.25 (\log(\text{dose}) = -1.39)$

### Approach :

1. Covariance matrix for the estimates:

  - The covariance matrix provides the variances and covariances of the parameter estimates $\beta_0, \beta_1$. The variances are the diagonal elements of the matrix, while the covariances are the off-diagonal elements. The correlation between the estimates can be calculated from their covariance:
  
      $\text{Corr}(\beta_0, \beta_1) = \frac{\text{Cov}(\beta_0, \beta_1)}{\sqrt{\text{Var}(\beta_0) \times \text{Var}(\beta_1)}}$
  
2. 95% confidence interval for the parameters:

  - The 95% confidence interval for the parameters can be calculated using the standard errors of the estimates. The confidence interval is given by:
  
      $\text{CI} = \hat{\beta} \pm Z \cdot \text{SE}(\hat{\beta})$
  
      where $\hat{\beta}$ is the estimate, $Z$ is the critical value for a 95% confidence interval, and $\text{SE}(\hat{\beta})$ is the standard error of the estimate.
  
3. 95% confidence interval for the tumor risk at dose $0.25 (\log({dose}) = -1.39)$:

  - Using the logistic regression equation:
   
      $\text{logit}(P) = \beta_0 + \beta_1 \times \log(dose)$
  
      we calculate the predicted probability $P$ at $\log(dose) = -1.39$
  
  - To find the confidence interval for $P$, we:
  
    - Calculate the standard error of $logit(P)$ by applying variance to the above logistic regression equation and taking a square root: 
    
        $\text{SE}(\beta_0+\beta_1 \times \log(dose)) = \sqrt{\text{Var}(\beta_0) + \text{Var}(\beta_1) \times \log(dose)^2 + 2 \times \text{Cov}(\beta_0, \beta_1) \times \log(dose)}$
    
    - Use it to derive the confidence interval and transform the interval back to the probability scale.
    
    
### Results :


```{r echo=FALSE, warning=FALSE}
# Covariance matrix
cov_matrix <- vcov(model_tumor)
#print the cov matrix into summary table
kable(cov_matrix, caption = "Covariance Matrix for Parameter Estimates")

# Correlation between beta_0 and beta_1
correlation <- cov_matrix[1, 2] / (sqrt(cov_matrix[1, 1] * cov_matrix[2, 2]))
#print correlation into summary table
kable(data.frame(Correlation = correlation), caption = "Correlation between beta_0 and beta_1")

# Confidence intervals for parameters
confint_params <- confint(model_tumor, level = 0.95)
#print the values into a summary table
kable(data.frame(Confidence_Interval = confint_params), caption = "95% Confidence Intervals for Parameters")


# Predicting risk at log(dose) = -1.39
log_dose_value <- -1.39
logit_p <- coef(model_tumor)[1] + coef(model_tumor)[2] * log_dose_value  # logit(P)
se_logit_p <- sqrt(cov_matrix[1, 1] + 2 * cov_matrix[1, 2] * log_dose_value + 
                   cov_matrix[2, 2] * ((log_dose_value)^2))
logit_p_ci_lb <- logit_p - (1.96 * se_logit_p)
logit_p_ci_ub <- logit_p + (1.96 * se_logit_p)

# Transforming logit(P) to probabilities
p <- exp(logit_p) / (1 + exp(logit_p))
p_ci_lb <- exp(logit_p_ci_lb) / (1 + exp(logit_p_ci_lb))
p_ci_ub <- exp(logit_p_ci_ub) / (1 + exp(logit_p_ci_ub))

#print values into summary table in one row with the confidence intervals
kable(data.frame(predicted_risk = p, confidence_interval_2.5 = p_ci_lb,  confidence_interval_97.5 = p_ci_ub), caption = "Predicted Risk and 95% Confidence Interval at log(dose) = -1.39")
#list(predicted_risk = p, confidence_interval = p_ci)

```

### Interpretation :

**I. Covariance Matrix and Correlation:**

  - Covariance Matrix:
  
      - The covariance matrix for the estimates $\beta_0$ and $\beta_1$ is:
    
      - Variance of $\beta_0$: 0.06633.
      
      - Variance of $\beta_1$: 0.00723.
      
      - Covariance between $\beta_0$ and $\beta_1$: 0.01436.
  
    
  - Correlation:
  
    - The correlation between $\beta_0$ and $\beta_1$ is 0.655 approximately, indicating a moderate positive correlation between the intercept and slope parameters. 
    
    - It implies that changes in the baseline log-odds of developing a tumor is associated with changes in the slope of the dose-response relationship.


**II. Confidence Intervals for Parameters:**

  - Confidence Interval for $\beta_0$:
  
    - The 95% confidence interval for $\beta_0$ is approximately [0.198, 1.213].
    
    - This means that the log-odds of developing a tumor at log(dose)=0 is likely within this range with 95% confidence.
    
  - Confidence Interval for $\beta_1$: 
  
    - The 95% confidence interval for $\beta_1$ is approximately [0.363, 0.699].
    
    - This means that each one unit increase in log(dose) increases the log-odds of developing a tumor by approximately 0.363 to 0.699 with 95% confidence.


**III. Predict Risk and Confidence Interval for P at a dose of 0.25 or log(dose) = -1.39:**
  
  - The predicted risk of developing a tumor at log(dose) = -1.39 is $P=0.409$ or 40.9%
   
  - The 95% confidence interval for the predicted risk is approximately [0.394, 0.588].
  
    - This means that at a dose of 0.25, the probability of developing a tumor is estimated to be between 39.4% and 58.8% with 95% confidence.
  


## Question 4

Perform a Wald-test of $H_0 : \beta_1 = 0$. Explain and show, with your own calculations based on R-output, how the test statistic is constructed. Interpret the result from the hypothesis test.

### Approach :

**1. Wald test:**

  - The wald test evaluates whether the slope of the parameter ($\beta_1$) is singificanlty different from zero.
  
  - The null hypothesis is $H_0 : \beta_1 = 0$, meaning that the dose log(dose) has no effect on the probability of developing a tumor.
  
  - The alternative hypothesis is $H_1 : \beta_1 \neq 0$, indicating that the dose log(dose) has an effect on the probability of developing a tumor.
  
**2. Test Statistic:**

  - The Wald test statistic is computed as:
  
    $W = \frac{\hat{\beta_1}}{\text{SE}(\hat{\beta_1})}$
  
  - Under the null hypothesis, $W$ follows a standard normal distribution.
  
**3. P-value:**

  - The p-value is calculated as:
  
    $P = 2 \times P(Z > |w|)$
    
  - If the p-value is less than the significance level (e.g., 0.05), we reject the null hypothesis, else we retain it.
  
### Results :

```{r echo=FALSE}

# Extract the estimate and standard error for beta_1
beta1 <- coef(model_tumor)[2]
se_beta1 <- summary_model_tumor$coefficients[, "Std. Error"][2]

# Wald test statistic
wald_statistic_beta1 <- beta1 / se_beta1

# P-value
p_value_beta1 <- 2 * (1 - pnorm(abs(wald_statistic_beta1)))

p_value_beta1 <- sprintf("%.12f", p_value_beta1)

#print the values into a summary table
kable(data.frame(Wald_Statistic = wald_statistic_beta1, P_Value = p_value_beta1), caption = "Wald Test Results")

```


**Significance of the Slope $\beta_1$:**

  - A large Wald statistic $W = 6.12$ means that the effect of the log(dose) on tumor probability is highly significant.

  - The p-value is very small (p<0.001) indicating concise evidence against the null hypothesis.
  
  - This means that the slope parameter $\beta-1$ is significantly different from zero, indicating that log(dose) has a significant effect on the probability of developing a tumor.


### Conclusion :

In conclusion, the Wald test shows that the slope parameter, $\beta_1$, is significantly different from zero. This indicates that $\log(\text{dose})$ has a statistically significant effect on the probability of developing a tumor. The extremely small p-value provides strong evidence against the null hypothesis ($H_0: \beta_1 = 0$). This confirms that the logistic regression model is appropriate for the data and that $\log(\text{dose})$ is a significant predictor of tumor probability.


## Question 5

Confirm whether the variance of parameter estimates is generally inversely proportional to the sample size.
Investigate the effect of sample size on the variance of parameter estimates by multiplying all counts in the table by 10, 100 and 1000. Report what you observe.

### Approach :


To investigate the effect of sample size on the variance of parameter estimates we:

  - Multiply all counts in the table by 10, 100, and 1000. 
  
  - Refit the logistic regression model to each new dataset.
  
  - Extract and compare the variances (diagonal elements of the covariance matrix) for $\beta_0$ and $\beta_1$.

### Results :

```{r echo=FALSE}
# Function to multiply counts and refit the model
fit_scaled_model <- function(scale) {
  scaled_tumor <- tumor * scale
  scaled_no_tumor <- no_tumor * scale
  scaled_total <- scaled_tumor + scaled_no_tumor
  model <- glm(cbind(scaled_tumor, scaled_no_tumor) ~ log_dose, family = binomial)
  variances <- diag(vcov(model))  # Extract variances of beta_0 and beta_1
  list(model = model, variances = variances)
}

# Fit models for scales of 10, 100, and 1000
scales <- c(10, 100, 1000)
results <- lapply(scales, fit_scaled_model)

# Extract variances for beta_0 and beta_1
variances <- sapply(results, function(res) res$variances)
rownames(variances) <- c("Variance (Intercept)", "Variance (log_dose)")
colnames(variances) <- paste0("Scale_", scales)

# Display the variances in a summary table
kable(variances, caption = "Variance of Parameter Estimates for Different Sample Sizes")
```

### Interpretation :

  - As the sample increases, the variances of both $\beta_0$ and $\beta_1$ decrease proportionally. This supports
  
      $\text{Variance} \propto \frac{1}{\text{Sample Size}}$
      
  - The variance of the intercept $\beta_0$ and $\beta_1$ decreases by a factor of 10, 100, and 1000 as the counts are scaled, showing that larger samples sizes result in more precise estimates.


### Conclusion :
  
As proven above, increasing the sample size reduces the variances of the parameter estimates. This demonstrates that both the intercept $\beta_0$ and slope $\beta_1$ benefit from increased precision with larger sample sizes. The inverse relationship between variance and sample size is a fundamental concept in statistics, and it is confirmed by the results of this analysis. Hence, the statement of the question is TRUE.
  
  
  
  
  
  
  
  