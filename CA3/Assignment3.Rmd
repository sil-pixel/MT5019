---
title: "Assignment III"
subtitle: "Log-Linear Models"
author: "Silpa Soni Nallacheruvu (19980824-5287) Hernan Aldana (20000526-4999)"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
library(knitr)
library(stringr)
```

---

# Exercise 3:1 (Higher Dimension Table)

## Question1: 

Fit several models in order to find a 'good' model for the given data collected from a birth clinic, which includes information on the mother's age, her smoking habits (number of cigarettes per day), gestational age (in days) and the survival status of the child.

```{r echo=FALSE, message=FALSE}
# Create the table data
data <- data.frame(
  `Mother's age` = c("< 30", "< 30", "< 30", "< 30", "30+", "30+", "30+", "30+"),
  `Smoking habits` = c("< 5", "< 5", "5+", "5+", "< 5", "< 5", "5+", "5+"),
  `Gestational age` = c("< 260", ">= 260", "< 260", ">= 260", "< 260", ">= 260", "< 260", ">= 260"),
  `Child survival - No` = c(50, 24, 9, 6, 41, 14, 4, 1),
  `Child survival - Yes` = c(315, 4012, 40, 459, 147, 1594, 11, 124)
)

# Generate the table
kable(data, caption = "Data from the study on the association of variables with child survival", col.names = c("Mother's age(X)", "Smoking habits(Y)", "Gestational age(Z)", "Child survival(V) - No", "Child survival(V) - Yes"))
```

### Approach: 

**1. Read the data:**

- The given dataset 'data_ca3.csv' contains the variables X, Y, Z, and V, along with their corresponding frequencies (n).
  
**2. Fit a saturated model:**

- Fit a saturated model which includes all four variables X, Y, Z, and V and all their interactions. It fits the data perfectly and serves as the reference model.
  
**3. Reduced Models:**

- We started by removing the 4 way interaction term from the saturated model.
- Then, we removed the 3 way interaction terms, then 2 way interaction terms, and finally we fit a model with the only the main effects.
- We removed interactions in a systematic way, where higher order interactions are removed before lower order interactions to evaluate the effect of each interaction term on the model.
  
**4. Model Comparison:**

- We compared the models using deviance, degrees of freedom, p-value, and AIC (Akaike Information Criterion) to evaluate the goodness of fit and complexity of the models.
- We calculated the p-value using the chi-square distribution calculated from the deviance and degrees of freedom since the deviance is derived from likelihood ratio statistic and it asympotically follows a chi-square distribution.

### R Output:

The following table contains the least and highest AIC models with their corresponding deviance, degrees of freedom, p-value, and AIC values for each combination of interactions.

```{r echo=FALSE, message=FALSE}

# Read the data
data3<-read.csv("data_ca3.csv")

# Create a data frame to store results
model_results <- data.frame(
  Model = character(),
  Deviance = numeric(),
  df = numeric(),
  `p-value` = numeric(),
  AIC = numeric()
)

# Define a function to extract model information
extract_model_info <- function(model, description) {
  if(is.null(model)) {
    return(data.frame(
      Model = '.',
      Deviance = '.',
      df = '.',
      `p-value` = '.',
      AIC = '.'
    ))
  }
  if(description == "XYZV") {
    return(data.frame(
      Model = description,
      Deviance = 0,
      df = 0,
      `p-value` = 1,
      AIC = AIC(model)
    ))
  }
  data.frame(
    Model = description,
    Deviance = model$deviance,
    df = model$df.residual,
    `p-value` = 1 - pchisq(model$deviance, model$df.residual),
    AIC = AIC(model)
  )
}

# Saturated model:
msat<-glm(n~x*y*z*v, family=poisson(link=log), data=data3)

# Add the saturated model to results
model_results <- rbind(model_results, extract_model_info(msat, "XYZV"))

# Function to generate and evaluate models with three-way interactions
evaluate_interactions <- function(data, terms, model_results) {
  # Loop through each combination of interactions
  highest_aic_combination <- c()
  lowest_aic_combination <- c()
  highest_model <- NULL
  lowest_model <- NULL
  for (combo in terms) {
    # Generate model formula
    interaction_terms <- paste(combo, collapse = "+")
    formula <- as.formula(paste("n ~", interaction_terms))
    
    # Fit the model
    model <- glm(formula, family = poisson(link = "log"), data = data)
    
    if(is.null(highest_model) || AIC(model) > AIC(highest_model)) {
      highest_model <- model
      highest_aic_combination <- combo
    }
    
    if(is.null(lowest_model) || AIC(model) < AIC(lowest_model)) {
      lowest_model <- model
      lowest_aic_combination <- combo
    }
  }
  # Append highest, filler, lowest AIC models
  model_results <- rbind(model_results, extract_model_info(highest_model, str_to_upper(paste(gsub("\\*", "", highest_aic_combination), collapse=','))))
  if(AIC(highest_model) != AIC(lowest_model)) {
    model_results <- rbind(model_results, extract_model_info(NULL, "."))
      model_results <- rbind(model_results, extract_model_info(lowest_model, str_to_upper(paste(gsub("\\*", "", lowest_aic_combination), collapse=','))))
  }
  
  return(model_results)
}

#generate interactions
terms <- c("x*y*z", "x*y*v", "x*z*v", "y*z*v")
model_results <- evaluate_interactions(data3, combn(terms, 4, simplify = FALSE), model_results)
model_results <- evaluate_interactions(data3, combn(terms, 3, simplify = FALSE), model_results)
model_results <- evaluate_interactions(data3, combn(terms, 2, simplify = FALSE), model_results)
model_results <- evaluate_interactions(data3, combn(terms, 1, simplify = FALSE), model_results)
terms2 <- c("x*y", "x*z", "x*v", "y*z", "y*v", "z*v")
model_results <- evaluate_interactions(data3, combn(terms2, 6, simplify = FALSE), model_results)
model_results <- evaluate_interactions(data3, combn(terms2, 5, simplify = FALSE), model_results)
model_results <- evaluate_interactions(data3, combn(terms2, 4, simplify = FALSE), model_results)
model_results <- evaluate_interactions(data3, combn(terms2, 3, simplify = FALSE), model_results)
model_results <- evaluate_interactions(data3, combn(terms2, 2, simplify = FALSE), model_results)
model_results <- evaluate_interactions(data3, combn(terms2, 1, simplify = FALSE), model_results)
terms3 <- c("x", "y", "z", "v")
model_results <- evaluate_interactions(data3, combn(terms3, 4, simplify = FALSE), model_results)

#print the model results into a table
kable(model_results, caption = "Model Comparison Results", align = c("l", "c", "c", "c", "c"), col.names = c("Model", "Deviance", "df", "p-value", "AIC"))
```

### Conclusion:

**1. Trends in AIC Across Models:**

  - Models with only main effects or single two-way interactions have high AICs, indicating poor fit, indicating that the main effects or single interactions alone are insufficient to explain the data.
  - Removing specific two-way and three-way interactions, such as ZV or XZV, significantly increases the AIC, highlighting the importance of these interaction terms of gestational age, mother's age and child survival for explaining the data.
  - The model with the lowest AIC is the one with interactions XY, XZ, XV, YV, ZV, suggesting it balances goodness-of-fit and model complexity most effectively. This model includes all two-way interactions except YZ, capturing significant dependencies among variables while avoiding overfitting. 

**2. Impact of Higher-Order Interactions:**

  - Models limited to single or lower-order interactions (e.g., X,Y,Z,V) exhibit low p-values compared to the models including the higher-order interactions (e.g., XYZ,XYV,YZV). 
  - The saturated model achieves perfect fit but at the cost of increased complexity, as reflected in its AIC.
  - Models with XYV, XZV as interactions have the highest p-values, indicating that removing some of the three-way interactions does not significantly reduce the model fit. These models are candidates for simpler yet statistically robust options.
  - The increasing trend of deviance and degrees of freedom (df) as the number of interactions decreases reflects the trade-off between model complexity and goodness-of-fit: simpler models fit the data less well but have more residual degrees of freedom.
  

### Interpretation:

  - For categorical variables like Mother’s Age, Smoking Habits, and Gestational Age, a two-way interaction like Smoking Habits $\times$ Gestational Age may explain most of the variation in child survival, while the three-way interaction Mother’s Age $\times$ Smoking Habits $\times$ Gestational Age contributes very little.
  - In practice, three-way interactions between variables like Mother’s Age, Smoking Habits, and Gestational Age often have small or negligible effects compared to the main effects and two-way interactions.
  
## Question2:   

Choose from your table a model with few parameters and a good fit. Describe the procedure to compare different models.

### Approach: 

To answer question 2, we aim to identify a good model that balances simplicity and fit. 

A good model should:

  - Explain the Relationships: Capture significant associations between variables.
  
  - Avoid Overfitting: Include only necessary interactions to prevent overfitting.
  
  - Optimize Fit: Minimize AIC and retain a good fit as assessed by likelihood ratio tests.

We can infer from the model comparison results that higher-order interactions (e.g., 3-way and 4-way) don't significantly contribute to explaining the relationships, as they may be too complex and difficult to interpret, as well as prone to overfitting. 

Therefore our focus will be on comparing models based on AIC, p-values, and goodness-of-fit, and identifying the model with the lowest AIC that balances fit and complexity.

To compare with the saturated model, we have chosen the model with the least AIC from the model comparison results of ordered AIC with a p-value of 0.9353 and 6 degrees of freedom. We will compare the saturated model with the chosen model by their AIC values.
  
```{r echo=FALSE, message=FALSE}
# Load the data
data3 <- read.csv("data_ca3.csv")

# Saturated model with all interactions
msat <- glm(n ~ x * y * z * v, family = poisson(link = "log"), data = data3)

# Reduced model with two-way interactions: XY, XZ, XV, YV, ZV
m1 <- glm(n ~ x * y + x * z + x * v + y * v + z * v, family = poisson(link = "log"), data = data3)

# Compare models using AIC and LRT
aic_results <- AIC(msat, m1)

# Output AIC results
kable(aic_results[2], caption = "AIC Comparison Results", col.names = c("AIC"))
```

### Conclusion:

**AIC Results:**

Saturate model (msat): AIC =  123.9732

Reduced model (m1): AIC =  113.7953

The reduced model (m1) has a lower AIC value compared to the saturated model, indicating a better balance between goodness-of-fit with fewer parameters.

**P-value:**

The null hypothesis in this case states that the reduced model (m1) is sufficient to explain the data.

We reject the null hypothesis if the p-value is less than the significance level (0.05), indicating that the reduced model is not sufficient to explain the data.

p-value = 0.9353

The p-value of the reduced model is 0.9353, which is much greater than the significance level (0.05). This indicates that we fail to reject the null hypothesis. Thus, the reduced model (m1) is a very good fit for the data, capturing the essential relationships between variables while maintaining simplicity and interpretability.

## Question3:

Interpret the model you chose. Which associations are significant? Quantify the associations with odds ratios together with confidence intervals.

### Interpretation of the model:

The reduced model (m1) with interactions XY, XZ, XV, YV, ZV was selected as the best model because:

  - It has the lowest AIC.

  - It retains statistically significant two way interactions that provide interpretative results about the relationships between variables.
  
  - Removing higher-order interactions (3-way and 4-way) did not significantly impact the model fit, as evidenced by the residual deviance and p-values.
  
  - The model captures the most critical associations between variables while maintaining simplicity and interpretability.


### Model coefficients:

The chosen model(m1) includes two-way interactions XY, XZ, XV, YV, ZV. This interactions highlight the relationships between:

  - Mother's age and smoking habits (XY): Suggests that smoking habits may vary across the age of mothers below and above 30 years.
  
  - Mother's age and gestational age (XZ): Suggests that gestational age may be influenced by maternal age.
  
  - Mother's age and child survival (XV): Suggests that child survival may depend if the maternal age is below or above 30 years.
  
  - Smoking habits and child survival (YV): Suggests there could be a direct impact of smoking habits on child survival rates.
  
  - Gestational age and child survival (ZV): Suggests that gestational age could be a factor in determining child survival rates.
  
The significance of these interactions is further evaluated by calculating by the p-values.

### Significance of Interactions:

The p-values for the interactions in the reduced model (m1) are as follows:

```{r echo=FALSE, message=FALSE}
# Print the p-values for the interactions in the reduced model
p_values <- summary(m1)$coefficients[, 4][6:10]
kable(p_values, caption = "P-values for Interactions in the Reduced Model", col.names = "P-value")
```
- The p-values for the interactions XY, XV, ZV are all less than 0.05, indicating that these interactions are statistically significant at the 5% significance level.
- This indicates that the interactions between Mother's age and smoking habits, Mother's age and child survival, and Gestational age and child survival are significant associations.
- The interactions XZ and YV have p-values greater than 0.05, suggesting that these interactions are statistically insignificant at the 5% significance level.
- This indicates that the interactions between Mother's age and gestational age, and Smoking habits and child survival are not significant associations.


### Odds Ratios and Confidence Intervals:

To quantify the associations between variables, we calculated the odds ratios and 95% confidence intervals for each significant interaction term in the model.

Compute Odds Ratios from Coefficients:

  - The coefficients of interaction terms in a log-linear model directly correspond to the logarithm of conditional odds ratio when a baseline level is defined for the variables involved in the interaction, assuming all other variables in the model are held constant.
  
  - Exponentiation of the coefficient converts the log-odds into conditional odds ratios, while keeping all other variables constant.
  
  - Compute the confidence interval of the conditional odds ratio by exponentiating the bounds of the confidence interval for the coefficient of the interaction term.


```{r echo=FALSE, message=FALSE}
# Calculate odds ratios and confidence intervals
odds_ratios <- exp(coef(m1)[6:10])
conf_intervals_lower <- exp(confint(m1)[6:10, 1])
conf_intervals_upper <- exp(confint(m1)[6:10, 2])
#print the odds ratios and confidence intervals
kable(cbind(odds_ratios, conf_intervals_lower, conf_intervals_upper), caption = "Conditional Odds Ratios and 95% Confidence Intervals", col.names = c("Cond. Odds Ratio", "Lower CI", "Upper CI"))
```

### Interpretation of Associations:

The conditional odds ratios and confidence intervals for each significant interaction term are interpreted as follows, while keeping all other variables constant and assuming a baseline level of zero ($\lambda_{00} = \lambda_{01} = \lambda_{10} = 0$)  for the interaction terms. This corresponds to the reference category of (*mother under 30 years old, smoked less than 5 cigarettes per day, gestational age under 260 days, and child did not survive*) for the variables involved in the interaction:

  - Mother's age and smoking habits (XY): Older maternal age reduces the odds of smoking habits by 33.7%.
  
  - Mother's age and gestational age (XZ): Older maternal age reduces the odds of a longer gestational age by 15.3%.
  
  - Mother's age and child survival (XV): Older maternal age reduces the odds of survival by 37.2%.
  
  - Smoking habits and child survival (YV): A 35.8% reduction in the odds of survival is observed with worsening smoking habits.
  
  - Gestational age and child survival (ZV): Child survival odds increase by a factor of 27.4 with longer gestational age.

### Conclusion:

- Quantitative Associations: 
  The Conditional odds ratios quantify the strength and direction of associations for maternal age, smoking habits, gestational age, and child survival. The association can be conversely interpreted by the reciprocal of the odds ratio for the inverse relationship between variables.
- Significant Predictors: 
  Variables with confidence intervals that exclude 1 (x:y, x:v, z:v) are statistically significant predictors.
- Practical Implications: 
  Mother's age and gestational age have the strongest influence on child survival, as shown by their conditional odds ratios.
  
  
## Question 4:

Fit a logistic regression model for the probability of child survival as a function of the explanatory variables. Interpret the results.

### Approach:

- Define the logistic regression Model:
  
  - Set Child Survival (V) as the response variable.
  
  - Use Mother's age (X), Smoking habits (Y), and Gestational age (Z) as explanatory variables.
  
  - Expanded the dataset into a binary response table based on the frequency of the child survival data to fit the logistic regression model.
  
- Fit the logistic Regression Model:

  - Use the glm() function with the family argument set to binomial(link = "logit") to fit the logistic regression model.
  
  - Fit the full model with all main effects and interactions, then remove insignificant interactions to simplify the model.
  
- Interpret the Results:
  
  - Examine the coefficients to determine the direction and strength of the relationships.
  
  - The coefficients of this logistic regression model represent the change in the log-odds of the response variable ($Y$) when the explanatory variables changes from 0 to 1, holding other variables constant.
  
  - For a i-th explanatory variable $X_i$, the coefficient of $X_i$ in the model is
  
  $\beta_i = \log\left(\frac{\text{Odds of } Y=1 \text{ when } X_i=1 \text{ and other variables held constant}}{\text{Odds of } Y=1 \text{ when } X_i=0 \text{ and other variables held constant}}\right)$ 
  
  - The conditional odds ratio for a response variable with respect to a i-th explanatory variable $X_i$ where other variables are constant is $e^{\beta_i}$.


  - Interpretation:
    - If $\beta_i$ > 0: The odds of the outcome are higher when $X_i$ = 1 compared to $X_i$ = 0, holding other variables constant.
    - If $\beta_i$ < 0: The odds of the outcome are lower when $X_i$ = 1 compared to $X_i$ = 0, holding other variables constant.
    - If $\beta_i$ = 0: $X_i$ has no effect on the odds of the outcome.

### Model Fitting and Interpretation:

```{r echo=FALSE, message=FALSE}
# Expand the dataset into a binary response table
binary_data <- data3[rep(1:nrow(data3), data3$n), ]  # Expand rows by frequency `n`
binary_data$n <- NULL  # Drop the frequency column, as it's no longer needed

# Full logistic regression model
logit_full <- glm(v ~ x + y + z + x:y + x:z + y:z, family = binomial(link = "logit"), data = binary_data)

cat("Full Logistic Model Summary:\n")
summary(logit_full)

# Remove insignificant interactions one by one
logit_step1 <- update(logit_full, . ~ . - x:y)
cat("\nLogistic Model after removing x:y interaction:\n")
summary(logit_step1)

logit_step2 <- update(logit_step1, . ~ . - x:z)
cat("\nLogistic Model after removing x:z interaction:\n")
summary(logit_step2)

logit_model <- update(logit_step2, . ~ . - y:z)
cat("\nFinal Logistic Model after removing y:z interaction:\n")
# Final reduced model
summary(logit_model)

# Calculate odds ratios and confidence intervals for final model
odds_ratios <- exp(coef(logit_model))
conf_intervals <- exp(confint(logit_model))

# Combine results into a table
logit_results <- data.frame(
  Variable = names(odds_ratios),
  Cond_Odds_Ratio = odds_ratios,
  Lower_CI = conf_intervals[, 1],
  Upper_CI = conf_intervals[, 2],
  P_Value = summary(logit_model)$coefficients[, 4]
)

# Print results
kable(logit_results, caption = "Logistic Regression Results: Cond. Odds Ratios, Confidence Intervals, and P-Values")

```

- We have chosen the final logistic regression model after removing the insignificant interactions based on the p-values and lowest AIC.

-**Model Summary:**

  - Null deviance: 1435.5 
  
  - Residual Deviance: 1084.8
    
    - a significant reduction in deviance from the null model, indicating a good fit.
    
  
-**Significant Predictors:**
    
  - Mother's age (X):
  
    - Conditional odds ratio: 0.63
    
    - The odds of child survival decrease by 37%. (1-0.63) when maternal age is more than 30 years, holding gestational age and smoking habits constant.
    
    - P-value: 0.009, indicating a statistically significant effect at the 5% significance level.
  
  - Gestational age (Z):
  
    - Conditional odds ratio: 27.38
    
    - Longer gestational age significantly increases the odds of child survival by 27 times, holding maternal age and smoking habits constant.
    
    - P-value: < 0.0001, indicating a highly significant effect at the 5% significance level.
    
-**Non-Significant Predictors:**
    
  - Smoking habits (Y):
  
    - Conditional odds ratio: 0.655
    
    - P-value: 0.107, indicating a non-significant effect at the 5% significance level.
    
    - The odds of child survival is suggested to decrease by 34.5% when smoking habits are more than 5 cigarettes per day, holding natural age and gestational age constant.
    
    - Although, it cannot be concluded for sure, since the p-value suggests that the odds ratio could have occurred due to random chance.

### Conclusion:

The logistic regression analysis provides valuable insight into the factors influencing child survival. The significant predictors in the model are Mother's age and Gestational age, which have a fairly strong impact on child survival odds.

- Model fit: The model shows a significant reduction in deviance from the null model, indicating a good fit. The AIC value of 1092.8 suggests that the model is relatively simple and provides a good balance between fit and complexity.
  
While smoking habits were not statistically significant in this analysis, further research may be needed to explore the impact of smoking on child survival in more detail.
  
  
## Question 5: 

Illustrate the relationship between the logistic regression model from question 4 and a corresponding log-linear model. Confirm that the two models gives identical estimates and standard errors of the corresponding parameters.

### Approach:

- **Log-Linear Model:**

  - Fit a log-linear model to analyze the relationship between the categorical variables Mother's age, Smoking habits, Gestational age, and Child survival to capture the relationships corresponding to the logistic regression model where Child survival is the response variable.
  
  - The log-linear model includes these additional terms to correspond to the logistic regression model - $v \sim x + y + z$ : all the interactions between the explanatory variables and each of the interaction between the response variable and the explanatory variables : $x*y, y*z, x*z, x*y*z, x*v, y*v, z*v$.

  
- **Relationship between Log-Linear and Logistic Regression Models:**

  - The log-linear model is used to analyze the relationship between categorical variables, while the logistic regression model is used to model the relationship between categorical response variables and explanatory variables.
  
  - Both models are generalized linear models (GLMs) that use the log link function to model the relationship between variables.
  
- **Comparison of Estimates and Standard Errors:**

  - We will compare the estimates and standard errors of the corresponding parameters from the log-linear and logistic regression models to confirm that they are identical.
  
  - We will extract the coefficients and standard errors from both models and compare them to verify the consistency of the results.
  

```{r echo=FALSE, message=FALSE}
# Extract coefficients and standard errors from the log-linear model
m_new <- glm(n ~ x + y + z + v + x*y + y*z + x*z + x*y*z + x*v + y*v + z*v, family = poisson(link = "log"), data = data3)
log_linear_coefs <- coef(m_new)[c("x:v", "y:v", "z:v")]
log_linear_se <- summary(m_new)$coefficients[, 2][c("x:v", "y:v", "z:v")]

# Extract coefficients and standard errors from the logistic regression model
# Fit a reduced logistic regression model
reduced_logit_model <- glm(v ~ x + y + z, family = binomial(link = "logit"), data = binary_data)
logit_coefs <- coef(logit_model)[c("x", "y", "z")]
logit_se <- summary(logit_model)$coefficients[, 2][c("x", "y", "z")]

# Create a data frame to compare the estimates and standard errors
comparison_results <- data.frame(
  Variable = c("Mother's age", "Smoking habits", "Gestational age"),
  Log_Linear_Est = log_linear_coefs,
  Logistic_Est = logit_coefs,
  Log_Linear_SE = log_linear_se,
  Logistic_SE = logit_se
)

# create a data frame for differences
difference_results <- data.frame(
  Variable = c("Mother's age", "Smoking habits", "Gestational age"),
  Est_Difference = format(abs(log_linear_coefs - logit_coefs), scientific = TRUE, digits = 6),
  SE_Difference = format(abs(log_linear_se - logit_se), scientific = TRUE, digits = 6)
)

# Print the comparison results in a table
kable(comparison_results, caption = "Comparison of Estimates and Standard Errors between Log-Linear and Logistic Regression Models", row.names = FALSE)

# print differences in estimates and standard errors
kable(difference_results, caption = "Differences in Estimates and Standard Errors between Log-Linear and Logistic Regression Models", row.names = FALSE)
```

### Interpretation:

While the estimation process is similar, the interpretation of parameters differs:

  - Logistic Regression: Coefficients represent the log of the conditional odds ratio of the outcome (child survival) for a one-unit change in the explanatory variable, holding all other variables constant.
  - Log-Linear Models: Coefficients represent the log of the conditional odds ratio for the interaction effects between variables, while other variables are held constant.  These coefficients are often interpreted relative to a defined baseline level set to zero in categorical variables.
  
### Justification:

  - Both the log-linear model and logistic regression model could be used to analyze the relationship between categorical variables and child survival.
  - The comparison of estimates and standard errors between the two models confirms that they yield identical results, justifying the consistency of the models we've chosen in capturing the relationships between variables.
  
**Strengths of Log-Linear Models:**

  - Ideal for exploring associations and interactions in multidimensional contingency tables.
  - Can handle higher-order interactions (e.g., three-way or four-way interactions) effectively.

**Strengths of Logistic Regression:** 

  - Better suited for predicting probabilities or understanding direct effects on a binary outcome.
  - More interpretable when the focus is on one specific outcome (e.g., child survival).
  
---  
  
