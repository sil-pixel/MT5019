---
title: "Assignment III"
subtitle: "Log-Linear Models"
author: "Silpa Soni Nallacheruvu (19980824-5287) Hernan Aldana (20000526-4999)"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
library(knitr)
library(stringr)
```

---

# Exercise 3:1 (Higher Dimension Table)

## Question1: 

Fit several models in order to find a 'good' model for the given data collected from a birth clinic, which includes information on the mother's age, her smoking habits (number of cigarettes per day), gestational age (in days) and the survival status of the child.

```{r echo=FALSE, message=FALSE}
# Create the table data
data <- data.frame(
  `Mother's age` = c("< 30", "< 30", "< 30", "< 30", "30+", "30+", "30+", "30+"),
  `Smoking habits` = c("< 5", "< 5", "5+", "5+", "< 5", "< 5", "5+", "5+"),
  `Gestational age` = c("< 260", ">= 260", "< 260", ">= 260", "< 260", ">= 260", "< 260", ">= 260"),
  `Child survival - No` = c(50, 24, 9, 6, 41, 14, 4, 1),
  `Child survival - Yes` = c(315, 4012, 40, 459, 147, 1594, 11, 124)
)

# Generate the table
kable(data, caption = "Data from the study on the association of variables with child survival", col.names = c("Mother's age(X)", "Smoking habits(Y)", "Gestational age(Z)", "Child survival(V) - No", "Child survival(V) - Yes"))
```

### Approach: 

**1. Read the data:**

- The given dataset 'data_ca3.csv' contains the variables X, Y, Z, and V, along with their corresponding frequencies (n).
  
**2. Fit a saturated model:**

- Fit a saturated model which includes all four variables X, Y, Z, and V and all their interactions. It fits the data perfectly and serves as the reference model.
  
**3. Reduced Models:**

- We started by removing the 4 way interaction term from the saturated model.
- Then, we removed the 3 way interaction terms, then 2 way interaction terms, and finally we fit a model with the only the main effects.
- We removed interactions in a systematic way, where higher order interactions are removed before lower order interactions to evaluate the effect of each interaction term on the model.
  
**4. Model Comparison:**

- We compared the models using deviance, degrees of freedom, p-value, and AIC (Akaike Information Criterion) to evaluate the goodness of fit and complexity of the models.
- We calculated the p-value using the chi-square distribution calculated from the deviance and degrees of freedom since the deviance is derived from likelihood ratio statistic and it asympotically follows a chi-square distribution.

### R Output:

The following table contains the least and highest AIC models with their corresponding deviance, degrees of freedom, p-value, and AIC values for each combination of interactions.

```{r echo=FALSE, message=FALSE}

# Read the data
data3<-read.csv("data_ca3.csv")

# Create a data frame to store results
model_results <- data.frame(
  Model = character(),
  Deviance = numeric(),
  df = numeric(),
  `p-value` = numeric(),
  AIC = numeric()
)

# Define a function to extract model information
extract_model_info <- function(model, description) {
  if(is.null(model)) {
    return(data.frame(
      Model = '.',
      Deviance = '.',
      df = '.',
      `p-value` = '.',
      AIC = '.'
    ))
  }
  if(description == "XYZV") {
    return(data.frame(
      Model = description,
      Deviance = 0,
      df = 0,
      `p-value` = 1,
      AIC = AIC(model)
    ))
  }
  data.frame(
    Model = description,
    Deviance = model$deviance,
    df = model$df.residual,
    `p-value` = 1 - pchisq(model$deviance, model$df.residual),
    AIC = AIC(model)
  )
}

# Saturated model:
msat<-glm(n~x*y*z*v, family=poisson(link=log), data=data3)

# Add the saturated model to results
model_results <- rbind(model_results, extract_model_info(msat, "XYZV"))

# Function to generate and evaluate models with three-way interactions
evaluate_interactions <- function(data, terms, model_results) {
  # Loop through each combination of interactions
  highest_aic_combination <- c()
  lowest_aic_combination <- c()
  highest_model <- NULL
  lowest_model <- NULL
  for (combo in terms) {
    # Generate model formula
    interaction_terms <- paste(combo, collapse = "+")
    formula <- as.formula(paste("n ~", interaction_terms))
    
    # Fit the model
    model <- glm(formula, family = poisson(link = "log"), data = data)
    
    if(is.null(highest_model) || AIC(model) > AIC(highest_model)) {
      highest_model <- model
      highest_aic_combination <- combo
    }
    
    if(is.null(lowest_model) || AIC(model) < AIC(lowest_model)) {
      lowest_model <- model
      lowest_aic_combination <- combo
    }
  }
  # Append highest, filler, lowest AIC models
  model_results <- rbind(model_results, extract_model_info(highest_model, str_to_upper(paste(gsub("\\*", "", highest_aic_combination), collapse=','))))
  if(AIC(highest_model) != AIC(lowest_model)) {
    model_results <- rbind(model_results, extract_model_info(NULL, "."))
      model_results <- rbind(model_results, extract_model_info(lowest_model, str_to_upper(paste(gsub("\\*", "", lowest_aic_combination), collapse=','))))
  }
  
  return(model_results)
}

#generate interactions
terms <- c("x*y*z", "x*y*v", "x*z*v", "y*z*v")
model_results <- evaluate_interactions(data3, combn(terms, 4, simplify = FALSE), model_results)
model_results <- evaluate_interactions(data3, combn(terms, 3, simplify = FALSE), model_results)
model_results <- evaluate_interactions(data3, combn(terms, 2, simplify = FALSE), model_results)
model_results <- evaluate_interactions(data3, combn(terms, 1, simplify = FALSE), model_results)
terms2 <- c("x*y", "x*z", "x*v", "y*z", "y*v", "z*v")
model_results <- evaluate_interactions(data3, combn(terms2, 6, simplify = FALSE), model_results)
model_results <- evaluate_interactions(data3, combn(terms2, 5, simplify = FALSE), model_results)
model_results <- evaluate_interactions(data3, combn(terms2, 4, simplify = FALSE), model_results)
model_results <- evaluate_interactions(data3, combn(terms2, 3, simplify = FALSE), model_results)
model_results <- evaluate_interactions(data3, combn(terms2, 2, simplify = FALSE), model_results)
model_results <- evaluate_interactions(data3, combn(terms2, 1, simplify = FALSE), model_results)
terms3 <- c("x", "y", "z", "v")
model_results <- evaluate_interactions(data3, combn(terms3, 4, simplify = FALSE), model_results)

#print the model results into a table
kable(model_results, caption = "Model Comparison Results", align = c("l", "c", "c", "c", "c"), col.names = c("Model", "Deviance", "df", "p-value", "AIC"))
```

### Conclusion:

**1. Trends in AIC Across Models:**

  - Models with only main effects or single two-way interactions have high AICs, indicating poor fit.
  - Removing specific two-way and three-way interactions, such as ZV or XZV, significantly increases the AIC, highlighting the importance of these interaction terms for explaining the data.
  - The model with the lowest AIC is the one with interactions XY, XZ, XV, YV, ZV, suggesting it balances goodness-of-fit and model complexity most effectively. This model includes all two-way interactions except YZ, capturing significant dependencies among variables while avoiding overfitting. 

**2. Impact of Higher-Order Interactions:**

  - Models including single or lower-order interactions (e.g., X,Y,Z,V) exhibit low p-values compared to the higher-order interactions (e.g., XYZ,XYV,YZV). 
  - The saturated model achieves perfect fit but at the cost of increased complexity, as reflected in its AIC.
  - Models with XYV, XZV as interactions have the highest p-values, indicating that removing some of the three-way interactions does not significantly reduce the model fit. These models are candidates for simpler yet statistically robust options.
  - The deviance and df of the models has an increasing trend as the number of interactions decreased.
  

### Interpretation:

  - Adding three-way interactions increases the number of parameters dramatically. 
  - For categorical variables like Mother’s Age, Smoking Habits, and Gestational Age, a two-way interaction like Smoking Habits $\times$ Gestational Age may explain most of the variation in child survival, while the three-way interaction Mother’s Age $\times$ Smoking Habits $\times$ Gestational Age contributes very little.
  - In practice, three-way interactions between variables like Mother’s Age, Smoking Habits, and Gestational Age often have small or negligible effects compared to the main effects and two-way interactions.
  
## Question2:   

Choose from your table a model with few parameters and a good fit. Describe the procedure to compare different models.


### Approach: 

To answer question 2, we aim to identify a good model that balances simplicity and fit. 

A good model should:

  - Explain the Relationships: Capture significant associations between variables.
  
  - Avoid Overfitting: Include only necessary interactions to prevent overfitting.
  
  - Optimize Fit: Minimize AIC and retain a good fit as assessed by likelihood ratio tests.

We can infer from the model comparison results that higher-order interactions (e.g., 3-way and 4-way) don't significantly contribute to explaining the relationships, as they may be too complex and difficult to interpret, as well as prone to overfitting. 

Therefore our focus will be on comparing models based on AIC, p-values, and goodness-of-fit, and p-values from Likelihood Ratio Test and identifying the model with the lowest AIC that balances fit and complexity.

### LRT Output:

We have chosen the Likelihood Ratio Test due to the nested nature of all models, where each model is a subset of the next model.

The Null Hypothesis of a LR test ($H_0$) : the simpler model is sufficient to explain the data.

Alternative Hypothesis ($H_A$) : the simpler model is not sufficient to explain the data. 

The p-value from the LR test will help us determine if the reduced model is significantly different from the saturated model where we reject the null hypothesis if the p-value is less than the significance level (0.05).

To compare with the saturated model, we have chosen the model with the least AIC from the model comparison results with a p-value of 0.9353 and 6 degrees of freedom. We will compare the saturated model with the chosen model using the Likelihood Ratio Test.
  
```{r echo=FALSE, message=FALSE}
# Load the data
data3 <- read.csv("data_ca3.csv")

# Saturated model with all interactions
msat <- glm(n ~ x * y * z * v, family = poisson(link = "log"), data = data3)

# Reduced model with two-way interactions: XY, XZ, XV, YV, ZV
m1 <- glm(n ~ x * y + x * z + x * v + y * v + z * v, family = poisson(link = "log"), data = data3)

# Compare models using AIC and LRT
aic_results <- AIC(msat, m1)
lrt_results <- anova(msat, m1, test = "LRT")

# Output AIC results
kable(aic_results[2], caption = "AIC Comparison Results", col.names = c("AIC"))

print("Likelihood Ratio Test Results:")
print(lrt_results)
```

### Conclusion:

**AIC Results:**

Saturate model (msat): AIC =  123.9732

Reduced model (m1): AIC =  113.7953

The reduced model (m1) has a lower AIC value compared to the saturated model, indicating a better balance between goodness-of-fit with fewer parameters.

**Residual Deviance:**

Saturation model (msat): 0.0000 (perfect fit)

Reduced model (m1): 1.8221

The residual deviance of the reduced model (m1) is slightly higher than the saturated model, which is expected as the reduced model has fewer parameters.

**P-value:**

p-value = 0.9353

The p-value from the Likelihood Ratio Test is 0.9353, which is greater than the statistical significant level 0.05. This indicates that the reduced model (m1) is a better fit compared to the saturated model (msat). The reduced model captures the essential relationships between variables while maintaining simplicity and interpretability.

## Question3:

Interpret the model you chose. Which associations are significant? Quantify the associations with odds ratios together with confidence intervals.

### Interpretation of the model:

The reduced model (m1) with interactions XY, XZ, XV, YV, ZV was selected as the best model because:

  - It has the lowest AIC.

  - It retains statistically significant two way interactions that provide interpretative results about the relationships between variables.
  
  - Removing higher-order interactions (3-way and 4-way) did not significantly impact the model fit, as evidenced by the residual deviance and p-values.
  
  - The model captures the most critical associations between variables while maintaining simplicity and interpretability.


### Model coefficients:

The chosen model(m1) includes significant two-way interactions XY, XZ, XV, YV, ZV. This interactions highlight the relationships between:

  - Mother's age and smoking habits (XY): Indicates that smoking habits vary significantly across maternal age groups.
  
  - Mother's age and gestational age (XZ): Suggests that gestational age may be influenced by maternal age.
  
  - Mother's age and child survival (XV): Indicates that child survival is critically dependent on maternal age.
  
  - Smoking habits and child survival (YV): Shows the direct impact of smoking habits on child survival rates.
  
  - Gestational age and child survival (ZV): Reinforces that gestational age is a key factor in determining child survival rates.

### Odds Ratios and Confidence Intervals:

To quantify the associations between variables, we calculated the odds ratios and 95% confidence intervals for each significant interaction term in the model.

Compute Odds Ratios from Coefficients:

  - The odds ratio for each coefficient in a logistic regression model represents the multiplicative change in the odds of the outcome for a one-unit increase in the predictor variable, holding all other variables constant.
  
  - Exponentiating the coefficient ($e^\beta$) converts the log-odds into odds ratios.
  
  - For each coefficient k, compute the confidence interval by exponentiating the bounds of the confidence interval for $\beta_k$.


```{r echo=FALSE, message=FALSE}
# Calculate odds ratios and confidence intervals
odds_ratios <- exp(coef(m1))
conf_intervals <- exp(confint(m1))
#print the odds ratios and confidence intervals
kable(cbind(odds_ratios, conf_intervals), caption = "Odds Ratios and 95% Confidence Intervals", col.names = c("Odds Ratio", "Lower CI", "Upper CI"))
```

### Interpretation of Associations:

The odds of child survival based on the odds ratios and confidence intervals for each significant interaction term are as follows:

  - Mother's age and smoking habits (XY): The odds of survival decrease by 33.7% for a certain smoking habit when maternal age increases.
  
  - Mother's age and gestational age (XZ): A 15.3% decrease in odds of survival is observed with longer gestational age for older mothers.
  
  - Mother's age and child survival (XV): Older maternal age reduces the odds of survival by 37.2%.
  
  - Smoking habits and child survival (YV): A 35.8% reduction in the odds of survival is observed with worsening smoking habits.
  
  - Gestational age and child survival (ZV): Child survival odds increase by a factor of 27.4 with longer gestational age.

### Conclusion:

- Quantitative Associations: 
  Odds ratios quantify the strength and direction of associations for maternal age, smoking habits, gestational age, and child survival.
- Significant Predictors: 
  Variables with confidence intervals that exclude 1 (x:y, x:v, z:v) are statistically significant predictors.
- Practical Implications: 
  Smoking habits and gestational age have the strongest influence on child survival, as shown by their odds ratios.
  
  
## Question 4:

Fit a logistic regression model for the probability of child survival as a function of the explanatory variables. Interpret the results.

### Approach:

- Define the logistic regression Model:
  
  - Set Child Survival (V) as the response variable.
  
  - Use Mother's age (X), Smoking habits (Y), and Gestational age (Z) as explanatory variables.
  
  - Expanded the dataset into a binary response table based on the frequency of the child survival data to fit the logistic regression model.
  
- Fit the logistic Regression Model:

  - Use the glm() function with the family argument set to binomial(link = "logit") to fit the logistic regression model.
  
- Interpret the Results:
  
  - Examine the coefficients to determine the direction and strength of the relationships.

### Interpretation of Results:

```{r echo=FALSE, message=FALSE}
# Expand the dataset into a binary response table
binary_data <- data3[rep(1:nrow(data3), data3$n), ]  # Expand rows by frequency `n`
binary_data$n <- NULL  # Drop the frequency column, as it's no longer needed

# Fit the logistic regression model
logit_model <- glm(v ~ x + y + z + x:y + x:z + y:z, 
                   family = binomial(link = "logit"), 
                   data = binary_data)

# Display the summary of the logistic regression model
summary(logit_model)

# Calculate odds ratios and confidence intervals
odds_ratios <- exp(coef(logit_model))
conf_intervals <- exp(confint(logit_model))

# Combine results into a data frame for interpretation
logit_results <- data.frame(
  Variable = names(odds_ratios),
  Odds_Ratio = odds_ratios,
  Lower_CI = conf_intervals[, 1],
  Upper_CI = conf_intervals[, 2],
  P_Value = summary(logit_model)$coefficients[, 4]
)

# Print the logistic regression results in a table
kable(logit_results, caption = "Logistic Regression Results: Odds Ratios, Confidence Intervals, and P-Values")
```

-**Model Summary:**

  - Null deviance: 1435.5 
  
  - Residual Deviance: 1083.6
    
    - a significant reduction in deviance from the null model, indicating a good fit.
    
  
-**Significant Predictors:**

 - Intercept:
 
    - Odds ratio = 6.37: The baseline of child survival when all predictors are zero.
    
    - Highly significant (p < 0.05).
    
  - Mother's age (X):
  
    - Estimate = -0.5893: Older mothers have decreased odds of their child survival.
    
    - Odds Ratio = 0.55: The odds of child survival decrease by 45% (for each year increase in maternal age(=1-0.55) for older mothers.
    
    - Significant (p = 0.01 < 0.05).
    
  - Gestational age (Z):
  
    - Estimate = 3.2479: Higher gestational age strongly increases the odds of child survival.
    
    - Odds Ratio = 25.74: For a longer gestational age, the odds of child survival are 25.74 higher for a one-unit increase in gestational age.
    
    - Highly significant (p < 0.05).
    
### Conclusion:

The logistic regression analysis provides valuable insight into the factors influencing child survival. The significant predictors in the model are Mother's age and Gestational age, which have a fairly strong impact on child survival odds.

- Mother's age: Older mothers have lower odds of child survival, with a 45% decrease in odds for each year increase in maternal age.

- Gestational age: Longer gestational age significantly increases the odds of child survival, with a 2474% increase in odds for each additional day of gestation.

- Model fit: The model shows a significant reduction in deviance from the null model, indicating a good fit. The AIC value of 1097.6 suggests that the model is relatively simple and provides a good balance between fit and complexity.
  
While smoking habits were not statistically significant in this analysis, further research may be needed to explore the impact of smoking on child survival in more detail.
  
  
## Question 5: 

Illustrate the relationship between the logistic regression model from question 4 and a corresponding log-linear model. Confirm that the two models gives identical estimates and standard errors of the corresponding parameters.

### Approach:

- **Log-Linear Model:**

  - Fit a log-linear model to analyze the relationship between the categorical variables Mother's age, Smoking habits, Gestational age, and Child survival to capture the relationships corresponding to the logistic regression model where Child survival is the response variable.
  
  - Use the glm() function with the family argument set to poisson(link = "log") to fit the log-linear model with interaction variables as $x * y * v, x * z * v, y * z * v$.
  
- **Relationship between Log-Linear and Logistic Regression Models:**

  - The log-linear model is used to analyze the relationship between categorical variables, while the logistic regression model is used to model the relationship between categorical response variables and explanatory variables.
  
  - Both models are generalized linear models (GLMs) that use the log link function to model the relationship between variables.
  
- **Comparison of Estimates and Standard Errors:**

  - We will compare the estimates and standard errors of the corresponding parameters from the log-linear and logistic regression models to confirm that they are identical.
  
  - We will extract the coefficients and standard errors from both models and compare them to verify the consistency of the results.
  
### Comparison of Estimates and Standard Errors:

```{r echo=FALSE, message=FALSE}
# Extract coefficients and standard errors from the log-linear model
m_new <- glm(n ~ x * y * v + x * z * v + y * z * v, family = poisson(link = "log"), data = data3)
log_linear_coefs <- coef(m_new)[c("x:v", "y:v", "v:z", "x:y:v", "x:v:z", "y:v:z")]
log_linear_se <- summary(m_new)$coefficients[, 2][c("x:v", "y:v", "v:z", "x:y:v", 
                                                 "x:v:z", "y:v:z")]

# Extract coefficients and standard errors from the logistic regression model
logit_coefs <- coef(logit_model)[c("x", "y", "z", "x:y", "x:z", "y:z")]
logit_se <- summary(logit_model)$coefficients[, 2][c("x", "y", "z", "x:y", "x:z"
                                                     , "y:z")]

# Create a data frame to compare the estimates and standard errors
comparison_results <- data.frame(
  Variable = c("Mother's age", "Smoking habits", "Gestational age", "Mother's age:Smoking habits", "Mother's age:Gestational age", "Smoking habits:Gestational age"),
  Log_Linear_Est = log_linear_coefs,
  Logistic_Est = logit_coefs,
  Log_Linear_SE = log_linear_se,
  Logistic_SE = logit_se
)

# Print the comparison results in a table
kable(comparison_results, caption = "Comparison of Estimates and Standard Errors between Log-Linear and Logistic Regression Models", row.names = FALSE)
```

### Interpretation:

While the estimation process is similar, the interpretation of parameters differs:

  - Logistic Regression: Coefficients represent changes in the log-odds of the child survival (binary outcome).
  - Log-Linear Models: Coefficients represent changes in the log of expected counts.

### Why Both Models Yield Consistent Estimates:

  - Both models rely on MLE, which is consistent and asymptotically efficient under regularity conditions.
  - The estimates and standard errors are derived using the Fisher information matrix, which is valid for both models.
  - Shared statistical frameworks (GLMs) ensure similar estimation methodologies, leading to consistent results.
  
### Justification:

  - Both the log-linear model and logistic regression model could be used to analyze the relationship between categorical variables and child survival.
  - The comparison of estimates and standard errors between the two models confirms that they yield identical results, providing consistent and reliable estimates of the relationships between variables.   
  
**Strengths of Log-Linear Models:**

  - Ideal for exploring associations and interactions in multidimensional contingency tables.
  - Can handle higher-order interactions (e.g., three-way or four-way interactions) effectively.

**Strengths of Logistic Regression:** 

  - Better suited for predicting probabilities or understanding direct effects on a binary outcome.
  - More interpretable when the focus is on one specific outcome (e.g., child survival).
