---
title: "Assignment IV"
subtitle: "Multiple Logistic Regression and Decision Tree"
author: "Silpa Soni Nallacheruvu (19980824-5287) Hernan Aldana (20000526-4999)"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
```

---

## Overview: 

Analysis of The ICU Study data (Hosmer & Lemeshow (1989): Applied Logistic Regression), with 200 patient records admitted to an Intensive Care Unit (ICU) using multiple logistic regression and decision tree models to identify factors that affect the survival of such patients.

# Exercise 4:1 (Multiple Logistic Regression)

##  Question 1

Report the model selection process briefly. Based on your chosen model, which factors affect the probability of not surviving? Report odds ratios with confidence intervals for the most important variables/factors, and interpret them. Use the variable names from the table (not V3, V4, etc.).

### Approach:

- Data Preparation:

  - Load the dataset and rename variables for clarity.
  
  - Combine categories for categorical variables if necessary.
  
- Model Fitting:

  - Fit an empty logistic regression model and a full logistic regression model to be used in the stepwise selection.
  - Here, Logistic Regression Model is used to predict the binary outcome of probability of not surviving based on multiple predictors.
  
- Model Selection Process:

  - Use a stepwise selection with AIC to identify a parsimonious model.
  
-	Analysis of the Final Model:

    - Extract coefficients, odds ratios, and their 95% confidence intervals for significant variables.

- Interpretation:

  - Interpret the results from the AIC, odds ratios, and confidence intervals, to determine the best model.

### Results:

Summary of the final model after performing stepwise selection using AIC:

```{r echo=FALSE}
data_ca4 <- read.csv("data_ca4.csv")

# Rename variables for clarity
colnames(data_ca4) <- c(
  "Patient", "Survival", "Age", "Sex", "Ethnicity", "TreatmentAtAdmission",
  "Cancer", "PreviousKidneyFailure", "Infection", "HeartLungTreatment",
  "BloodPressure", "HeartRate", "AdmittedToICUWithin6Months",
  "TypeOfAdmission", "FractureOfNeckOrSpine", "BloodOxygen",
  "BloodPH", "BloodCarbonDioxide", "BloodBicarbonate",
  "BloodCreatine", "ConsciousnessLevel"
)

# Combine categories for Ethnicity (example)
data_ca4$Ethnicity[data_ca4$Ethnicity > 1] <- 0

# Fit models
m_empty <- glm(Survival ~ 1, family = binomial, data = data_ca4)   # Empty model
m_full <- glm(Survival ~ ., family = binomial, data = data_ca4)    # Full model

# Stepwise model selection using AIC
m_step <- step(m_empty, scope = list(lower = m_empty, upper = m_full), direction = "both", trace = FALSE)

# Summary of the final model
summary(m_step)

```


**Model selection:**

- The final logistic regression model includes the following variables: ConsciousnessLevel, TypeOfAdmission, Age, Cancer, Patient, BloodCarbonDioxide, BloodPH, and BloodPressure.

- These variables were selected using a stepwise AIC, which ensures a balance between model complexity and goodness of fit.

**Significant Variables:**

- Variables with a p-value < 0.05 are considered significant predictors of survival:

  - ConsciousnessLevel
  
  - TypeOfAdmission
  
  - Age
  
  - Cancer
  
  - BloodCarbonDioxide, 
  
  - BloodPH

- Patient variable was exlcluded from the final model as the ID code was not significant. 
- BloodPressure was also excluded from the final model as it was not significant according to the p-value.

### Odds Ratios and Confidence Intervals:

Here's the final report after extracting odds ratios and confidence intervals for significant variables:

```{r echo=FALSE}

# Extract Odds Ratios and Confidence Intervals
odds_ratios <- exp(coef(m_step))  # Odds Ratios
conf_intervals <- exp(confint(m_step))  # 95% Confidence Intervals

# Combine results into a data frame for clarity
results <- data.frame(
  Variable = names(odds_ratios),
  OddsRatio = odds_ratios,
  `CI Lower` = conf_intervals[, 1],
  `CI Upper` = conf_intervals[, 2]
)

# Filter significant variables (p-value < 0.05)
significant_vars <- summary(m_step)$coefficients
significant_vars <- significant_vars[significant_vars[, 4] < 0.05, ]

# Final table for reporting
final_results <- results %>%
  filter(Variable %in% rownames(significant_vars))

final_results
```

1. ConsciousnessLevel: 

   - Odds Ratio: 13.75 (CI: 4.31-65.28)
   
   - Patients who are unconscious or in a coma have a significantly higher probability of not surviving compared to those who are conscious.
   
2. TypeOfAdmission:

   - Odds Ratio: 21.25 (CI: 4.36-189.15)
   
   - Acute admissions are associated with a significantly higher probability of not surviving compared to non-acute admissions.
   
3. Age:

   - Odds Ratio: 1.04 (CI: 1.01-1.07)
   
   - For each additional year of age, the odds of not surviving increase by 4%.

4. Cancer:

   - Odds Ratio: 10.37 (CI: 1.95-66.54)
   
   - Patients with cancer have over 10 times higher odds of not surviving compared to those without cancer.
   
5. BloodCarbonDioxide:

   - Odds Ratio: 0.085 (CI: 0.008-0.55)
   
   - Lower blood carbon dioxide levels significantly reduce the odds of not surviving.
  
6. BloodPH:

   - Odds Ratio: 8.07 (CI: 1.4-53.60)
   
   - Lower blood pH levels significantly increase the odds of not surviving.

### Conclusion:

The selected model indicates that factors such as consciousness level, type of admission, age, cancer, blood carbon dioxide, and blood pH are significant predictors of survival. Patients who are unconscious, have acute admissions, are older, have cancer, and have abnormal blood gas levels are at higher risk of not surviving. These results can help identify high-risk patients and improve treatment strategies to increase survival rates.


## Question 2

How well does your chosen model fit the data? In assignment 3, deviance was used to assess model fit. However, for individual-level data, deviance is unsuitable. Instead, perform the Hosmer-Lemeshow goodness-of-fit test using the recommended R code.

### Approach:

- Understand the Hosmer-Lemeshow Test:

  - The Hosmer-Lemeshow test evaluates whether the observed event rates matches the expected probabilities predicted by the model.
  
  - The null hypothesis is that the model fits the data well (a high p-value suggests no evidence of poor fit).

- Implementation:

  - Use the function for the test ResourceSelection::hoslem.test()
  
  - Calculate the predicted probabilities from the final model.
  
  - Specify the predicted probabilities from the final model and the actual outcomes while performing Hosmer-Lemeshow test (m_step and Survival respectively).
  
  - 10 is selected as the number of groups for the test because dividing by deciles is a common choice and it was sufficient for this dataset of 200 observations.
  
### Hosmer-Lemeshow Test Results:

Here are the results of the Hosmer-Lemeshow goodness-of-fit test:

```{r echo=FALSE}
# Calculate predicted probabilities
predicted_probs <- predict(m_step, type = "response")

# Perform Hosmer-Lemeshow goodness-of-fit test
hoslem_test <- ResourceSelection::hoslem.test(data_ca4$Survival, predicted_probs, g = 8)

# Display test results
hoslem_test
```

**Interpretation:**

  1. The p-value of 0.8998 tells us not reject the null hypothesis that the model fits the data well. 
  
### Conclusion:

The Hosmer-Lemeshow test indicates that the model fits the data well. The observed event rates are consistent with the expected probabilities predicted by the model. This suggests that the model is a good fit for the data and can be used to make accurate predictions about survival probabilities.


## Question 3

Create a confusion matrix for the chosen model. Calculate the accuracy, sensitivity, specificity, and positive and negative predictive values. Interpret the results.

### Approach:

- Understand the Confusion Matrix:

  - A confusion matrix is a table that summarizes the performance of a classification model.
  
  - It shows the number of true positives, true negatives, false positives, and false negatives.
  
- Implementation:

  - Threshold of 0.5 is used to classify the predicted probabilities into binary outcomes since it is the default threshold for logistic regression.
  
  - Calculate the accuracy, sensitivity, specificity, and positive and negative predictive values from the confusion matrix.
  
  - Here, the positive of the model is "Not Survived" and the negative is "Survived".

**Sensitivity: ** Sensitivity (True Positive Rate) measures the proportion of actual positive cases that are correctly identified by the model. 

**Specificity: ** Specificity (True Negative Rate) measures the proportion of actual negative cases that are correctly identified by the model.

  
### Results:

```{r echo=FALSE, message=FALSE}
# Predicted probabilities
predicted_classes <- ifelse(predicted_probs > 0.5, 1, 0)

# Confusion Matrix
conf_matrix <- table(data_ca4$Survival, predicted_classes)

# Calculate performance metrics
accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)
sensitivity <- conf_matrix[1, 1] / sum(conf_matrix[1, ])
specificity <- conf_matrix[2, 2] / sum(conf_matrix[2, ])

# Print the confusion matrix
rownames(conf_matrix) <- c("Actual Not Survived", "Actual Survived")
knitr::kable(conf_matrix, caption = "Confusion Matrix", col.names = c("Predicted Not Survived", "Predicted Survived"), row.names = TRUE)

# Display performance metrics
knitr::kable(data.frame(
  Metric = c("Accuracy", "Sensitivity", "Specificity"),
  Value = c(accuracy, sensitivity, specificity)
), caption = "Performance Metrics", col.names = c("Metric", "Value"))

```


### Interpretation:

**Accuracy:** The model has an accuracy of 0.865, meaning that it correctly predicted 86.5% of the cases.

**Sensitivity:** The sensitivity of 0.96875 indicates that the model correctly identified 96.9% of the actual non-survivors.

**Specificity:** The specificity of 0.45 suggests that the model correctly identified 45% of the actual survivors.

### Conclusion:

The confusion matrix and performance metrics provide insights into the model's predictive accuracy. The model has a high sensitivity, indicating that it is highly effective at identifying non-survivors. However, the specificity is relatively low, suggesting that the model has difficulty identifying actual survivors. The results tell us that the model can be used for rule-out purposes of non-survivors. Further optimization may be needed to improve the model's sensitivity for predicting survivors.


## Question 4

Create plots of ROC curves for the chosen model. Calculate the AUC for the full model and two more models. Choose the best model based on the AUC. Interpret the results.

### Approach:

- Understand ROC Curves and AUC:

  - ROC curves are used to evaluate the performance of classification models by plotting the true positive rate against the false positive rate.
  
  - The AUC (Area Under the Curve) summarizes the ROC curve, with higher values indicating better model performance.
  
- Implementation:

  - Compare the AUC values for the full model and two additional models  and the model with the highest AUC is considered the best model for predicting survival probabilities.

  - The two additional models used for comparison with the full model are derived by removing a significant variable and a non-significant variable from the full model (such as ConsciousnessLevel and Blood Pressure as mentioned in Q1).
  
  - Thereby, we can compare the significance of these variables in predicting survival probabilities by observing the change in AUC values using ROC curves.
  
      - Full Model: 
      
      $Survival \sim ConsciousnessLevel + TypeOfAdmission + Age + Cancer + BloodCarbonDioxide + BloodPH + Patient + BloodPressure$
  
      - Model A: Exclude Blood Pressure from the full model.

      $Survival \sim ConsciousnessLevel + TypeOfAdmission + Age + Cancer + BloodCarbonDioxide + BloodPH + Patient$
      
      - Model B : Exclude ConsciousnessLevel from the full model.

      $Survival \sim TypeOfAdmission + Age + Cancer + BloodCarbonDioxide + BloodPH + Patient + BloodPressure$
  
### Results:

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.width=6, fig.height=4, fig.align='center'}
# Load the pROC package
library(pROC)
full_model_roc <- roc(data_ca4$Survival, predicted_probs)
plot(full_model_roc, col = "blue", main = "ROC Curves", legacy.axes = TRUE)

formula_full_model <- "Survival ~ ConsciousnessLevel + TypeOfAdmission + Age + Cancer + Patient + BloodCarbonDioxide + BloodPH + BloodPressure"
formula_model_a <- "Survival ~ ConsciousnessLevel + TypeOfAdmission + Age + Cancer + BloodCarbonDioxide + BloodPH + Patient"
formula_model_b <- "Survival ~ TypeOfAdmission + Age + Cancer + BloodCarbonDioxide + BloodPH + Patient + BloodPressure"

# Additional models
# Model 1: Remove non-significant variable BloodPressure
m_significant_1 <- glm(formula_model_a, family = binomial, data = data_ca4)
predicted_probs_significant_1 <- predict(m_significant_1, type = "response")
significant_model_roc_1 <- roc(data_ca4$Survival, predicted_probs_significant_1)
plot(significant_model_roc_1, col = "red", add = TRUE, legacy.axes = TRUE)

# Model 2: Remove significant variable ConsciousnessLevel 
m_significant_2 <- glm(formula_model_b, family = binomial, data = data_ca4)
predicted_probs_significant_2 <- predict(m_significant_2, type = "response")
significant_model_roc_2 <- roc(data_ca4$Survival, predicted_probs_significant_2)
plot(significant_model_roc_2, col = "green", add = TRUE, legacy.axes = TRUE)
legend("bottomright", legend = c("Full Model", "Model A", "Model B"), col = c("blue", "red", "green"), lty = 1)

# Calculate AUC
auc_full_model <- auc(full_model_roc)
auc_model_1 <- auc(significant_model_roc_1)
auc_model_2 <- auc(significant_model_roc_2)
knitr::kable(data.frame(
  Model = c("Full Model", "Model A", "Model B"),
  AUC = c(auc_full_model, auc_model_1, auc_model_2)
), caption = "AUC Values", col.names = c("Model", "AUC"))
```

### Interpretation:

- The full model has the highest AUC of 0.873, indicating that it has a better predictive performance for survival probabilities compared to the other models.
- Model A, which excludes Blood Pressure, has an AUC of 0.871 is quite close to the full model, whereas Model B, which excludes ConsciousnessLevel, has an AUC of 0.804, which has a bigger difference with full model compared to the Model B.
- This can be interpreted as Blood Pressure does not significantly contribute to the patient survival prediction, as its exclusion does not significantly affect the AUC of the model.
- Whereas ConsciousnessLevel is a significant predictor of survival, and its exclusion has a more significant impact on the model's predictive performance.
- The AUC of all three models are much higher than 0.5 (random guessing) and can be considered effective for predicting survival probabilities, with the full model being the best among them.

## Question 5

Perform Leave One Out Cross Validation (LOOCV) for the above full model and the additional models. Calculate the LOOCV-adjusted AUC for the three models and compare with the results from question 4. Which model indicates the best predictive performance?

### Approach:

- Understand Leave One Out Cross Validation (LOOCV):

  - LOOCV is a technique for assessing the predictive performance of a model by training on all but one observation and testing on the left-out observation.
  
  - The AUC values from LOOCV provide an estimate of the model's performance on unseen data.
  
  - The three models used in question 4 are evaluated using LOOCV to determine the best predictive performance.
  
### Results:

```{r echo=FALSE, message=FALSE, warning=FALSE}

# Create a vector for the predicted values
predprob_LOOCV <- numeric(nrow(data_ca4))  

# function for LOOCV
loocv <- function(formula) {
  predprob_LOOCV <- numeric(nrow(data_ca4))  
  for (i in 1:nrow(data_ca4)) {
    # Create training and validation sets
    data_training <- data_ca4[-i,]
    data_validation <- data_ca4[i, ,drop=FALSE]
    # Fit the model on the training data
    loocv_model <- glm(formula, family=binomial, data=data_training)
    # Predict the value for the held-out observation
    predprob_LOOCV[i] <- predict(loocv_model, newdata = data_validation, type = "response")
  }
  return(predprob_LOOCV)
}

# LOOCV for the full model
predprob_LOOCV_full_model <- loocv(formula_full_model)
loocv_full_model_roc <- roc(data_ca4$Survival, predprob_LOOCV_full_model)
predprob_LOOCV_model_a <- loocv(formula_model_a)
loocv_model_a_roc <- roc(data_ca4$Survival, predprob_LOOCV_model_a)
predprob_LOOCV_model_b <- loocv(formula_model_b)
loocv_model_b_roc <- roc(data_ca4$Survival, predprob_LOOCV_model_b)

# print the results of AUC for three models
knitr::kable(data.frame(
  Model = c("Full Model", "Model A", "Model B"),
  AUC = c(auc(loocv_full_model_roc), auc(loocv_model_a_roc), auc(loocv_model_b_roc))
), caption = "LOOCV-Adjusted AUC Values", col.names = c("Model", "AUC"))
```


### Interpretation:

- According the LOOCV-adjusted AUC values, Model A has the highest AUC of 0.824, followed by the Full Model with an AUC of 0.819, and Model B with an AUC of 0.756.

- LOOCV-adjusted AUC is a more reliable AUC as it is calculated by leaving out one observation at a time and predicted using the model trained on the remaining data.

- Model A having a higher AUC than full model indicates that the full model was overfitted in the previous analysis, and Model A is the best model for predicting survival probabilities.

- This means that Blood Pressure is not required to be added as a explanatory variable while predicting the survival probabilities of the patients.

- The AUC of Model B has further decreased compared to the previous analysis, suggesting that the previous Model B was also overfitted and that there is a bigger impact of the ConsciousnessLevel of the patient in predicting their survival than previously thought.

- This signifies that ConsciousnessLevel of the patient is very essential to predict the survival probability.



# Exercise 4:2 (Decision Tree)

## Question 1

Fit a decision tree model to the ICU data using the rpart package. Use the same predictors as in the multiple logistic regression model. Plot the decision tree and interpret the results.

### Approach:

To classify the survival status of patients admitted to the ICU, a decision tree model is applied. Decision trees offer an interpretative way to identify key predictors and their thresholds that affect survival. For this task, the following steps were followed:

1. **Model Fitting**:
   - A decision tree model was fit using the ICU dataset.
   - The response variable (`v2`) indicates survival (0 = survived, 1 = not survived).
   - Predictor variables include age, blood pressure, consciousness level, and other clinical factors.

2. **Parameters Adjusted**:
   - **Splitting criterion**: Both "information" and "gini" criteria were tested to evaluate their effects on splits.
   - **Complexity parameter (`cp`)**: Different values were used to control the depth of the tree and prevent overfitting.

3. **Visualization**:
   - Tree diagrams were created using the `rpart.plot` package to assess the structure and interpretability of the models.

4. **Tree Selection**:
   - The selecion criteria for the best tree includes:
   
     - **Interpretability**: The tree should be easy to interpret and explain. Trees with fewer splits are preferred.
     - **Relevance of Splits**: Identify important predictors of survival.
     - **Clinical Relevance**: For ICU patients, identifying key factors affecting survival is crucial.

5. **Variables of Interest**:
   - **`v21` (Consciousness level)**: The primary variable splitting the data in both trees, indicating its importance for predicting survival.
   - **`v11` (Blood pressure)**: A secondary variable used in the second tree, capturing further distinctions in survival likelihood.


### Results:

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.width=10, fig.height=12, fig.align='center'}

library(rpart)
library(rpart.plot)

# The package "rpart.plot" must have been installed
#Fitting a decision tree model
tm1 <- rpart(Survival ~.,method = "class", data=data_ca4, parms = list(split = "gini"), cp = 0.1)
# use split = "gini" or split = "information"
# cp "complexity parameter" can be adjusted

tm2 <- rpart(Survival ~.,method = "class", data=data_ca4, parms = list(split = "gini"), cp = 0.01)

tm3 <- rpart(Survival ~.,method = "class", data=data_ca4, parms = list(split = "gini"), cp = 0.001)

tm4 <- rpart(Survival ~.,method = "class", data=data_ca4, parms = list(split = "information"), cp = 0.1)

tm5 <- rpart(Survival ~.,method = "class", data=data_ca4, parms = list(split = "information"), cp = 0.01)

tm6 <- rpart(Survival ~.,method = "class", data=data_ca4, parms = list(split = "information"), cp = 0.001)


# Set up a multi-panel layout with 3 rows and 2 columns
par(mfrow = c(3, 2), mar = c(4, 4, 2, 1))  # 3 rows, 2 columns, adjust margins for spacing

# Plot each pair of trees side by side
rpart.plot(tm1, main = "Gini, cp = 0.1", digits = 3)
rpart.plot(tm4, main = "Info Gain, cp = 0.1", digits = 3)

rpart.plot(tm2, main = "Gini, cp = 0.01", digits = 3)
rpart.plot(tm5, main = "Info Gain, cp = 0.01", digits = 3)

rpart.plot(tm3, main = "Gini, cp = 0.001", digits = 3)
rpart.plot(tm6, main = "Info Gain, cp = 0.001", digits = 3)

# Reset layout to default
par(mfrow = c(1, 1))
```
\newpage

### Interpretation and selection:

**Simplicity and Predictors**

- All trees highlight Consciousness Level as the primary predictor of survival.

- Trees with cp = 0.1 are the simplest, making them easy to interpret, but they do not refine predictions beyond the primary split.

- Trees with cp = 0.01 add an extra split, offering more detailed classification without adding excessive complexity.

- Trees with cp = 0.001 are the most complex, with multiple splits that may lead to overfitting.

**Compare splitting Criteria**

- **Gini index** tends to favor splits with balanced distributions of the target classes, which may better handle uncertainty in clinical settings.

- **Information gain** focuses on maximizing the reduction in entropy, which could lead to splits that are sightly more biased but more efficient for some datasets.

**Simplicity vs refinement**

- a tree with cp = 0.01 is a good compromise, as it adds refinement without sacrificing interpretation, and without risking overfitting.

- Among the splitting methods with same cp, the preference depends on the dataset:

  - If the data has imbalanced classes, gini might be better.
  - If entropy reduction is preferred, Information could perform better.
  
### Conclusion:

Based on the uploading plots and logic: 

- **Best tree:** Gini, cp=0.01.
The tree with cp = 0.01 using Gini Index was selected because it offers a good trade-off between interpretation and predictive refinement. Consciousness Level remains the primary driver of survival predictions, while the additional split on Blood Pressure refines subgroups with varying survival probabilities. This tree is simple enough for clinical use while providing actionable insights.

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.width=5, fig.height=3, fig.align='center'}
rpart.plot(tm2, main = "Gini, cp = 0.01", digits = 3)
```

\newpage

## Question 2

In this question we have to assess how well the chosen tree predicts for Survival, using the AUC metric, to measure predictive performance. Finally we will compare the AUC of the decision tree with the AUC of the logistic regression model.

### Approach:

- **Calculate Predicted Probabilities**:
  Use the `predict()` function to calculate the predicted probabilities for the chosen decision tree model.
- **Calculate AUC**:
  Use the `roc()` function from the `pROC` package to calculate the AUC for the decision tree model.
- **Logistic Regression Model**:
  Compare the AUC of the decision tree with the AUC of the logistic regression model to assess predictive performance.

### Results:

```{r echo=FALSE, message=FALSE, warning=FALSE}
library(pROC)
library(kableExtra)

# Predicted probabilities for the decision tree
pred_tm2 <- predict(tm2, type = "prob")[, 2]  # Probabilities of 'Not Survived'

# AUC for the decision tree
auc_tm2 <- auc(data_ca4$Survival, pred_tm2)

# Predicted probabilities for the logistic regression model
pred_logit <- predict(m_step, type = "response") 

# AUC for the logistic regression model
auc_logit <- auc(data_ca4$Survival, pred_logit)
# Create the comparison table
auc_comparison <- data.frame(
  Model = c("Decision Tree (Gini, cp = 0.01)", "Logistic Regression"),
  AUC = c(auc_tm2, auc_logit)
)

# Generate a formatted table
kable(auc_comparison) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), 
                full_width = FALSE, 
                position = "center") %>%
  row_spec(1, color = "black", background = "white") %>%
  row_spec(2, color = "black", background = "white")
```

1. **Decision Tree AUC**: The decision tree model with Gini Index and cp = 0.01 has an AUC of 0.8118, indicating a good predictive performance.

2. **Logistic Regression AUC**: The logistic regression model has an AUC of 0.8729, which is higher than the decision tree model.

### Interpretation:

1. **Decision Tree vs. Logistic Regression**:
The decision tree is slightly less accurate but offers better interpretation making it a useful option when simplicity is a priority. (For real-time clinical applications)

2. **Logistic Regression**: 
Logistic regression achieves higher predictive accuracy, which may be preferable if accuracy outweighs interpretation.

In conclusion based on the AUC values, the logistic regression model is the better-performing model for predicting survival. However, the decision tree remains as a valuable option for its interpretation and simplicity, especially in clinical settings where understanding the decision-making process is crucial.

## Question 3

Calculate a LOOCV-corrected AUC for the decision tree model and comment on the result.

### Approach:

- **LOOCV for Decision Tree**:

  Perform Leave One Out Cross Validation (LOOCV) for the decision tree model to calculate the LOOCV-adjusted AUC.
- We have chosen the above mentioned best decision tree model with Gini Index and cp = 0.01 for this analysis.

### Results:

```{r echo=FALSE, message=FALSE, warning=FALSE}
# Create a vector for the predicted values
predprob_tm_LOOCV <- numeric(nrow(data_ca4)) 

for (i in 1:nrow(data_ca4)) {
  # Create training and validation sets
  data_training_tree <- data_ca4[-i,]
  data_validation_tree <- data_ca4[i, ,drop=FALSE]
  # Fit the model on the training data
  tm_loocv <- rpart(Survival ~.,method = "class", data=data_training_tree, parms = list(split = "gini"), cp = 0.01)
  # Predict the value for the held-out observation
  predprob_tm_LOOCV[i] <- predict(tm_loocv, newdata = data_validation_tree, type = "prob")[,2]
}

roc(data_ca4$Survival, predprob_tm_LOOCV)
```


### Interpretation:

- The LOOCV-adjusted AUC is 0.6284 which is a significant decrease from the original AUC of 0.8115. This indicates that the decision tree model may be overfitting the data, leading to a lower predictive performance on unseen data. 
- Hence, when comparing the LOOCV-adjusted AUCs of the decision tree model and the logistic regression model, the logistic regression model remains the better-performing model and more reliable for predicting survival probabilities on unseen data.
- The decision tree model may benefit from additional tuning or simplification to enhance its predictive performance and avoid overfitting.
- This could be due to the presence of the parameters BloodPressure and Patient ID in the model, which may not be significant predictors of survival and could be contributing to overfitting.