---
title: "Assignment IV"
subtitle: "Multiple Logistic Regression and Decision Tree"
author: "Silpa Soni Nallacheruvu (19980824-5287) Hernan Aldana (20000526-4999)"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(knitr)
```

---

## Overview: 

Analysis of The ICU Study data (Hosmer & Lemeshow (1989): Applied Logistic Regression), with 200 patient records admitted to an Intensive Care Unit (ICU) using multiple logistic regression and decision tree models to identify factors that affect the survival of such patients.

# Exercise 4:1 (Multiple Logistic Regression)

##  Question 1

Report the model selection process briefly. Based on your chosen model, which factors affect the probability of not surviving? Report odds ratios with confidence intervals for the most important variables/factors, and interpret them. Use the variable names from the table (not V3, V4, etc.).

### Approach:

- Data Preparation:

  - Load the dataset and rename variables for clarity.
  
  - Combine categories for categorical variables if necessary:
    - For "Ethnicity," combined "Black" and "Other" into a single category to address low cell counts. 
    - Rationale: To ensure sufficient representation in each category for statistical analysis.

- **Model Fitting**:
  - Fit an empty logistic regression model and a full logistic regression model to be used in the stepwise selection.
  - Use stepwise selection in a both-directional manner to identify a parsimonious model using AIC as the selection criterion. 

- **Patient ID Variable**:
  - "Patient" variable (ID) was excluded as it does not have a meaningful interpretation. 

- **Analysis of the Final Model**:
  - Extract coefficients, odds ratios, and their 95% confidence intervals for significant variables.
  - Significant variables were identified as those with p-values less than 0.05.

- **Interpretation**:
  - Interpret the results from the AIC, odds ratios, and confidence intervals to determine the most impactful predictors.


### Results:

Summary of the final model after performing stepwise selection using AIC:

```{r echo=FALSE}
# Load dataset
data_ca4 <- read.csv("data_ca4.csv")

# Rename variables for clarity
colnames(data_ca4) <- c(
  "Patient", "Survival", "Age", "Sex", "Ethnicity", "TreatmentAtAdmission",
  "Cancer", "PreviousKidneyFailure", "Infection", "HeartLungTreatment",
  "BloodPressure", "HeartRate", "AdmittedToICUWithin6Months",
  "TypeOfAdmission", "FractureOfNeckOrSpine", "BloodOxygen",
  "BloodPH", "BloodCarbonDioxide", "BloodBicarbonate",
  "BloodCreatine", "ConsciousnessLevel"
)

# Combine categories for Ethnicity
data_ca4$Ethnicity[data_ca4$Ethnicity > 1] <- 0  # Combine "Black" and "Other" [Comment 1.2]

# Exclude "Patient" variable as it is not meaningful [Comment 2.3]
data_ca4 <- data_ca4 %>% select(-Patient)

# Fit empty and full logistic regression models
m_empty <- glm(Survival ~ 1, family = binomial, data = data_ca4)  # Empty model
m_full <- glm(Survival ~ ., family = binomial, data = data_ca4)   # Full model

# Stepwise model selection using AIC in both directions [Comment 2.1]
m_step <- step(m_empty, scope = list(lower = m_empty, upper = m_full), direction = "both", trace = FALSE)

# Extract summary of the final model
summary(m_step)

```


**Model selection:**

- The final logistic regression model includes the following variables: ConsciousnessLevel, TypeOfAdmission, Age, Cancer, BloodCarbonDioxide, BloodPH, and BloodPressure.

- These variables were selected using a stepwise AIC, which ensures a balance between model complexity and goodness of fit.

**Significant Variables:**

- Variables with a p-value < 0.05 are considered significant predictors of survival:

  - ConsciousnessLevel
  
  - TypeOfAdmission
  
  - Age
  
  - Cancer
  
  - BloodCarbonDioxide, 
  
  - BloodPH

-The variable BloodPressure was not statistically significant (p = 0.103152) and is retained in the model for completeness.

### Odds Ratios and Confidence Intervals:

Here's the final report after extracting odds ratios and confidence intervals for significant variables:

```{r echo=FALSE}

# Extract Odds Ratios and Confidence Intervals for significant variables
odds_ratios <- exp(coef(m_step))  # Odds Ratios
conf_intervals <- exp(confint(m_step))  # 95% Confidence Intervals

# Filter significant variables (p-value < 0.05)
significant_vars <- summary(m_step)$coefficients
significant_vars <- significant_vars[significant_vars[, 4] < 0.05, ]

# Combine results into a table
results <- data.frame(
  Variable = names(odds_ratios),
  OddsRatio = odds_ratios,
  `CI Lower` = conf_intervals[, 1],
  `CI Upper` = conf_intervals[, 2]
) %>%
  filter(Variable %in% rownames(significant_vars)) %>% 
  filter(Variable != "(Intercept)")  # Exclude the intercept explicitly

# Display results
kable(results, caption = "Odds Ratios and Confidence Intervals for Significant Variables")
```

1.	ConsciousnessLevel:

	-	Odds Ratio: 10.42 (CI: 3.64–43.47)
	-	 Patients who are unconscious or in a coma are over 10 times more likely to not survive compared to those who are conscious.
	
2.	TypeOfAdmission:

	-	Odds Ratio: 15.69 (CI: 3.84–133.69)
	-	 Acute admissions are associated with approximately 16 times higher odds of not surviving compared to non-acute admissions.

3.	Age:
	
	-	Odds Ratio: 1.04 (CI: 1.01–1.07)
	-	 For every additional year of age, the odds of not surviving increase by 4%.

4.	Cancer:
	
	-	Odds Ratio: 8.71 (CI: 1.66–53.52)
	-	 Patients with cancer are over 8 times more likely to not survive compared to those without cancer.
	
5.	BloodCarbonDioxide:
	-	Odds Ratio: 0.10 (CI: 0.01–0.63)
	-	 Higher levels of blood carbon dioxide significantly reduce the odds of not surviving.
	
6.	BloodPH:
	-	Odds Ratio: 6.11 (CI: 1.10–37.71)
	-	 Patients with higher blood pH levels have an increased risk of not surviving

### Conclusion:

The model indicates that factors such as consciousness level, type of admission, age, cancer, blood carbon dioxide, and blood pH are significant predictors of survival. Patients who are unconscious, have acute admissions, are older, have cancer, have lower levels of blood carbon dioxide, and have higher levels of blood pH are at a higher risk of not surviving. This analysis helps identify high-risk patients and can guide clinical interventions to improve survival rates by targeting these critical factors.


## Question 2

How well does your chosen model fit the data? In assignment 3, deviance was used to assess model fit. However, for individual-level data, deviance is unsuitable. Instead, perform the Hosmer-Lemeshow goodness-of-fit test using the recommended R code.

### Approach:

- Understand the Hosmer-Lemeshow Test:

  - The Hosmer-Lemeshow test evaluates whether the observed event rates matches the expected probabilities predicted by the model.
  
  - The null hypothesis is that the model fits the data well (a high p-value suggests no evidence of poor fit).

- Implementation:

  - Use the function for the test ResourceSelection::hoslem.test()
  
  - Calculate the predicted probabilities from the final model.
  
  - Specify the predicted probabilities from the final model and the actual outcomes while performing Hosmer-Lemeshow test (m_step and Survival respectively).
  
  - 10 is selected as the number of groups for the test because dividing by deciles is a common choice and it was sufficient for this dataset of 200 observations.
  
### Hosmer-Lemeshow Test Results:

Here are the results of the Hosmer-Lemeshow goodness-of-fit test:

```{r echo=FALSE}
# Calculate predicted probabilities
predicted_probs <- predict(m_step, type = "response")

# Perform Hosmer-Lemeshow goodness-of-fit test
hoslem_test <- ResourceSelection::hoslem.test(data_ca4$Survival, predicted_probs, g = 10)

# Display test results
hoslem_test
```

**Interpretation:**

  1. The p-value of 0.8404 tells us to not reject the null hypothesis that the model fits the data well. 
  
### Conclusion:

The Hosmer-Lemeshow test with 10 groups (df = 8) yielded a chi-squared statistic of 4.1809 and a p-value of 0.8404. Since the p-value is much greater than 0.05, we fail to reject the null hypothesis that the model fits the data well. This indicates that the predicted probabilities align closely with the observed survival outcomes, confirming the adequacy of the logistic regression model for this dataset.

## Question 3

Create a confusion matrix for the chosen model. Calculate the accuracy, sensitivity, specificity, and positive and negative predictive values for three values of threshold. Describe and explain the result.

### Approach:

- Understand the Confusion Matrix:

  - A confusion matrix is a table that summarizes the performance of a classification model.
  
  - It shows the number of true positives, true negatives, false positives, and false negatives.
  
- Implementation:

  - Threshold of 0.3, 0.5 and 0.7 are used to classify the predicted probabilities into binary outcomes to view the range of sensitivity and specificity.
  
  - Calculate the accuracy, sensitivity, specificity, and positive and negative predictive values from the confusion matrix for each threshold.
  
  - Here, the positive of the model is "Not Survived" and the negative is "Survived".

**Accuracy:** Accuracy measures the proportion of correct predictions made by the model.

$\text{Accuracy} = \frac{\text{True Positives (TP)} + \text{True Negatives (TN)}}{\text{Total Predictions (TP + TN + FP + FN)}}$

**Sensitivity: ** Sensitivity (True Positive Rate) measures the proportion of actual positive cases that are correctly identified by the model. 

$\text{Sensitivity (True Positive Rate)} = \frac{\text{True Positives (TP)}}{\text{True Positives (TP)} + \text{False Negatives (FN)}}$

**Specificity: ** Specificity (True Negative Rate) measures the proportion of actual negative cases that are correctly identified by the model.

$\text{Specificity (True Negative Rate)} = \frac{\text{True Negatives (TN)}}{\text{True Negatives (TN)} + \text{False Positives (FP)}}$


**Threshold = 0.5:** The model classifies predictions with probabilities greater than or equal to 0.5 as positive (predicts “not survive”) and those below 0.5 as negative (predicts “survive”).

  
### Results:

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.align='left'}
create_conf_matrix <- function(threshold) {
  # Predicted probabilities
  predicted_classes <- ifelse(predicted_probs > threshold, 1, 0)
  
  # Confusion Matrix
  conf_matrix <- table(1-data_ca4$Survival, 1-predicted_classes)

  # Print the confusion matrix
  rownames(conf_matrix) <- c("Actual Not Survived", "Actual Survived")
  return (conf_matrix)
}
conf_matrix_0.3 <- create_conf_matrix(0.3)
kable(conf_matrix_0.3, caption = paste("Confusion Matrix at threshold=", 0.3), col.names = c("Predicted Not Survived", "Predicted Survived"), row.names = TRUE)
conf_matrix_0.5 <- create_conf_matrix(0.5)
kable(conf_matrix_0.5, caption = paste("Confusion Matrix at threshold=", 0.5), col.names = c("Predicted Not Survived", "Predicted Survived"), row.names = TRUE)
conf_matrix_0.7 <- create_conf_matrix(0.7)
kable(conf_matrix_0.7, caption = paste("Confusion Matrix at threshold=", 0.7), col.names = c("Predicted Not Survived", "Predicted Survived"), row.names = TRUE)
```



```{r echo=FALSE, message=FALSE, fig.align='right'}
create_perf_metrics <- function(threshold, conf_matrix) {
    # Calculate performance metrics
  accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)
  sensitivity <- conf_matrix[1, 1] / sum(conf_matrix[1, ])
  specificity <- conf_matrix[2, 2] / sum(conf_matrix[2, ])

  # Display performance metrics
  kable(data.frame(
    Metric = c("Accuracy", "Sensitivity", "Specificity"),
    Value = c(accuracy, sensitivity, specificity)
  ), caption = paste("Performance Metrics at threshold=", threshold), col.names = c("Metric", "Value"))
}
# call at threshold 0.3
create_perf_metrics(0.3, conf_matrix_0.3)
# call at threshold 0.5
create_perf_metrics(0.5, conf_matrix_0.5)
# call at threshold 0.7
create_perf_metrics(0.7, conf_matrix_0.7)
```


### Interpretation:

**Threshold = 0.3:**

**Accuracy:** The model has an accuracy of 0.835, meaning that it correctly predicted 83.5% of the cases.

**Sensitivity:** The sensitivity of 0.625 indicates that the model correctly identified 62.5% of the actual non-survivors.

**Specificity:** The specificity of 0.8875 suggests that the model correctly identified 88.75% of the actual survivors.

**Threshold = 0.5:**

**Accuracy:** The model has an accuracy of 0.865, meaning that it correctly predicted 86.5% of the cases.

**Sensitivity:** The sensitivity of 0.45 indicates that the model correctly identified 45% of the actual non-survivors.

**Specificity:** The specificity of 0.96875 suggests that the model correctly identified 96.9% of the actual survivors.

**Threshold = 0.7:**

**Accuracy:** The model has an accuracy of 0.84, meaning that it correctly predicted 84% of the cases.

**Sensitivity:** The sensitivity of 0.25 indicates that the model correctly identified 25% of the actual non-survivors.

**Specificity:** The specificity of 0.9875 suggests that the model correctly identified 98.75% of the actual survivors.

**Impact of Threshold:**

- Thresholds control the trade-off between sensitivity and specificity:
	- A lower threshold (e.g., 0.3) typically increases sensitivity because more cases are classified as “not survive,” but it may decrease specificity.
	- A higher threshold (e.g., 0.7) typically increases specificity because fewer cases are classified as “not survive,” but sensitivity may decrease.
	- Compared to 0.3 and 0.7 thresholds, the accuracy is highest at the threshold of 0.5, which is the default threshold for binary classification.

### Conclusion:

The confusion matrix and performance metrics provide insights into the model's predictive accuracy. In general over the three thresholds, the model has a high specificity, indicating that it is highly effective at identifying survivors. However, the sensitivity is relatively low, suggesting that the model has difficulty identifying actual non-survivors. 


## Question 4

Create plots of ROC curves for the chosen model. Calculate the AUC for the full model and two more models. Choose the best model based on the AUC. 

### Approach:

- Understand ROC Curves and AUC:

  - ROC curves are used to evaluate the performance of classification models by plotting the true positive rate against the false positive rate.
  
  - The AUC (Area Under the Curve) summarizes the ROC curve, with higher values indicating better model performance.
  
- Implementation:

  - Compare the AUC values for the full model and two additional models  and the model with the highest AUC is considered the best model for predicting survival probabilities.

  - The two additional models used for comparison with the full model are derived by removing a significant variable and a non-significant variable from the full model (such as ConsciousnessLevel and Blood Pressure as mentioned in Q1).
  
  - Thereby, we can compare the significance of these variables in predicting survival probabilities by observing the change in AUC values using ROC curves.
  
      - Full Model: 
      
      $Survival \sim ConsciousnessLevel + TypeOfAdmission + Age + Cancer + BloodCarbonDioxide + BloodPH + Patient + BloodPressure$
  
      - Model A: Exclude Blood Pressure from the full model.

      $Survival \sim ConsciousnessLevel + TypeOfAdmission + Age + Cancer + BloodCarbonDioxide + BloodPH + Patient$
      
      - Model B : Exclude ConsciousnessLevel from the full model.

      $Survival \sim TypeOfAdmission + Age + Cancer + BloodCarbonDioxide + BloodPH + Patient + BloodPressure$
  
### Results:

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.width=8, fig.height=6, fig.align='center'}
# Load the pROC package
library(pROC)
full_model_roc <- roc(data_ca4$Survival, predicted_probs)
plot(full_model_roc, col = "blue", main = "ROC Curves", legacy.axes = TRUE)

formula_full_model <- "Survival ~ ConsciousnessLevel + TypeOfAdmission + Age + Cancer+ BloodCarbonDioxide + BloodPH + BloodPressure"
formula_model_a <- "Survival ~ ConsciousnessLevel + TypeOfAdmission + Age + Cancer + BloodCarbonDioxide + BloodPH"
formula_model_b <- "Survival ~ TypeOfAdmission + Age + Cancer + BloodCarbonDioxide + BloodPH + BloodPressure"

# Additional models
# Model 1: Remove non-significant variable BloodPressure
m_significant_1 <- glm(formula_model_a, family = binomial, data = data_ca4)
predicted_probs_significant_1 <- predict(m_significant_1, type = "response")
significant_model_roc_1 <- roc(data_ca4$Survival, predicted_probs_significant_1)
plot(significant_model_roc_1, col = "red", add = TRUE, legacy.axes = TRUE)

# Model 2: Remove significant variable ConsciousnessLevel 
m_significant_2 <- glm(formula_model_b, family = binomial, data = data_ca4)
predicted_probs_significant_2 <- predict(m_significant_2, type = "response")
significant_model_roc_2 <- roc(data_ca4$Survival, predicted_probs_significant_2)
plot(significant_model_roc_2, col = "green", add = TRUE, legacy.axes = TRUE)
legend("bottomright", legend = c("Full Model", "Model A", "Model B"), col = c("blue", "red", "green"), lty = 1)

# Calculate AUC
auc_full_model <- auc(full_model_roc)
auc_model_1 <- auc(significant_model_roc_1)
auc_model_2 <- auc(significant_model_roc_2)
knitr::kable(data.frame(
  Model = c("Full Model", "Model A", "Model B"),
  AUC = c(auc_full_model, auc_model_1, auc_model_2)
), caption = "AUC Values", col.names = c("Model", "AUC"))
```

### Model Selection:

- The full model has the highest AUC of 0.873, indicating that it has a better predictive performance for survival probabilities compared to the other models.
- Model A, which excludes Blood Pressure, has an AUC of 0.871 is quite close to the full model, whereas Model B, which excludes ConsciousnessLevel, has an AUC of 0.804, which has a bigger difference with full model compared to the Model B.
- This can be interpreted as Blood Pressure does not significantly contribute to the patient survival prediction, as its exclusion does not significantly affect the AUC of the model.
- Whereas ConsciousnessLevel is a significant predictor of survival, and its exclusion has a more significant impact on the model's predictive performance.
- The AUC of all three models are much higher than 0.5 (random guessing) and can be considered effective for predicting survival probabilities, with the full model being the best among them.

## Question 5

Perform Leave One Out Cross Validation (LOOCV) for the above full model and the additional models. Calculate the LOOCV-adjusted AUC for the three models and compare with the results from question 4. Which model indicates the best predictive performance?

### Approach:

- Understand Leave One Out Cross Validation (LOOCV):

  - LOOCV is a technique for assessing the predictive performance of a model by training on all but one observation and testing on the left-out observation.
  
  - The AUC values from LOOCV provide an estimate of the model's performance on unseen data.
  
  - The three models used in question 4 are evaluated using LOOCV to determine the best predictive performance.
  
### Results:

```{r echo=FALSE, message=FALSE, warning=FALSE}

# Create a vector for the predicted values
predprob_LOOCV <- numeric(nrow(data_ca4))  

# function for LOOCV
loocv <- function(formula) {
  predprob_LOOCV <- numeric(nrow(data_ca4))  
  for (i in 1:nrow(data_ca4)) {
    # Create training and validation sets
    data_training <- data_ca4[-i,]
    data_validation <- data_ca4[i, ,drop=FALSE]
    # Fit the model on the training data
    loocv_model <- glm(formula, family=binomial, data=data_training)
    # Predict the value for the held-out observation
    predprob_LOOCV[i] <- predict(loocv_model, newdata = data_validation, type = "response")
  }
  return(predprob_LOOCV)
}

# LOOCV for the full model
predprob_LOOCV_full_model <- loocv(formula_full_model)
loocv_full_model_roc <- roc(data_ca4$Survival, predprob_LOOCV_full_model)
predprob_LOOCV_model_a <- loocv(formula_model_a)
loocv_model_a_roc <- roc(data_ca4$Survival, predprob_LOOCV_model_a)
predprob_LOOCV_model_b <- loocv(formula_model_b)
loocv_model_b_roc <- roc(data_ca4$Survival, predprob_LOOCV_model_b)

# print the results of AUC for three models
knitr::kable(data.frame(
  Model = c("Full Model", "Model A", "Model B"),
  AUC = c(auc(loocv_full_model_roc), auc(loocv_model_a_roc), auc(loocv_model_b_roc))
), caption = "LOOCV-Adjusted AUC Values", col.names = c("Model", "AUC"))
```


### Interpretation:

- According the LOOCV-adjusted AUC values, Model A has the highest AUC of 0.824, followed by the Full Model with an AUC of 0.819, and Model B with an AUC of 0.756.

- LOOCV-adjusted AUC is a more reliable AUC as it is calculated by leaving out one observation at a time and predicted using the model trained on the remaining data.

- Model A having a higher AUC than full model indicates that the full model was overfitted in the previous analysis, and Model A is the best model for predicting survival probabilities.

- This means that Blood Pressure is not required to be added as a explanatory variable while predicting the survival probabilities of the patients and removing it from the model improves the predictive performance.

- The AUC of Model B has further decreased compared to the previous analysis, suggesting that the previous Model B was also overfitted and that there is a bigger impact of the ConsciousnessLevel of the patient in predicting their survival than previously thought.

- This signifies that ConsciousnessLevel of the patient is very essential to predict the survival probability.

---


\newpage 

# Exercise 4:2 (Decision Tree)

## Question 1

Fit a decision tree model to the ICU data using the rpart package. Use the same predictors as in the multiple logistic regression model. Plot the decision tree and interpret the results.

### Approach:

To classify the survival status of patients admitted to the ICU, a decision tree model is applied. Decision trees offer an interpretative way to identify key predictors and their thresholds that affect survival. For this task, the following steps were followed:

1. **Model Fitting**:
   - A decision tree model was fit using the ICU dataset.
   - The response variable (`v2`) indicates survival (0 = survived, 1 = not survived).
   - The algorithm was allowed to automatically determine the most suitable variables for splitting at each node based on the Gini Index or Information Gain criteria.

2. **Parameters Adjusted**:
   - **Splitting criterion**: Both "information" and "gini" criteria were tested to evaluate their effects on splits.
   - **Complexity parameter (`cp`)**: Different values were used to control the depth of the tree and prevent overfitting.

3. **Visualization**:
   - Tree diagrams were created using the `rpart.plot` package to assess the structure and interpretability of the models.

4. **Tree Selection**:
   - The selecion criteria for the best tree includes:
   
     - **Interpretability**: The tree should be easy to interpret and explain. Trees with fewer splits are preferred.
     - **Relevance of Splits**: Identify important predictors of survival.
     - **Clinical Relevance**: For ICU patients, identifying key factors affecting survival is crucial.

5. **Variables of Interest**:
   - **`v21` (Consciousness level)**: The primary variable splitting the data in both trees, indicating its importance for predicting survival.
   - **`v11` (Blood pressure)**: A secondary variable used in the second tree, capturing further distinctions in survival likelihood.


### Results:

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.width=10, fig.height=12, fig.align='center'}

library(rpart)
library(rpart.plot)

# The package "rpart.plot" must have been installed
#Fitting a decision tree model
tm1 <- rpart(Survival ~.,method = "class", data=data_ca4, parms = list(split = "gini"), cp = 0.1)
# use split = "gini" or split = "information"
# cp "complexity parameter" can be adjusted

tm2 <- rpart(Survival ~.,method = "class", data=data_ca4, parms = list(split = "gini"), cp = 0.01)

tm3 <- rpart(Survival ~.,method = "class", data=data_ca4, parms = list(split = "gini"), cp = 0.001)

tm4 <- rpart(Survival ~.,method = "class", data=data_ca4, parms = list(split = "information"), cp = 0.1)

tm5 <- rpart(Survival ~.,method = "class", data=data_ca4, parms = list(split = "information"), cp = 0.01)

tm6 <- rpart(Survival ~.,method = "class", data=data_ca4, parms = list(split = "information"), cp = 0.001)


# Set up a multi-panel layout with 3 rows and 2 columns
par(mfrow = c(3, 2), mar = c(4, 4, 2, 1))  # 3 rows, 2 columns, adjust margins for spacing

# Plot each pair of trees side by side
rpart.plot(tm1, main = "Gini, cp = 0.1", digits = 3)
rpart.plot(tm4, main = "Info Gain, cp = 0.1", digits = 3)

rpart.plot(tm2, main = "Gini, cp = 0.01", digits = 3)
rpart.plot(tm5, main = "Info Gain, cp = 0.01", digits = 3)

rpart.plot(tm3, main = "Gini, cp = 0.001", digits = 3)
rpart.plot(tm6, main = "Info Gain, cp = 0.001", digits = 3)

# Reset layout to default
par(mfrow = c(1, 1))
```
\newpage

### Interpretation and selection:

**Simplicity and Predictors**

- All trees highlight Consciousness Level as the primary predictor of survival.

- Trees with cp = 0.1 are the simplest, making them easy to interpret, but they do not refine predictions beyond the primary split.

- Trees with cp = 0.01 in the gini case, these trees added several splits beyond the primary split on Conciusness Level, providing more refined predictions.

- Trees with cp = 0.001 are the most complex tested trees , with multiple splits that may lead to overfitting.

**Compare splitting Criteria**

- **Gini index** tends to favor splits with balanced distributions of the target classes, which may better handle uncertainty in clinical settings.

- **Information gain** focuses on maximizing the reduction in entropy, which could lead to splits that are sightly more biased but more efficient for some datasets.

**Simplicity vs refinement**

- a tree with cp = 0.01 is a good compromise, as it adds refinement without sacrificing interpretation, and without risking overfitting.

- Among the splitting methods with same cp, the preference depends on the dataset:

  - If the data has imbalanced classes, gini might be better.
  - If entropy reduction is preferred, Information could perform better.
  
### Conclusion:

Based on the uploading plots and logic: 

- **Best tree:** Gini, cp=0.01.
The tree with cp = 0.01 using Gini Index was selected because it offers a good trade-off between interpretation and predictive refinement among different complexities. However, both gini index and information criteria achieved the same tree with a cp od 0.01. Consciousness Level remains the primary driver of survival predictions, while the additional split on Blood Pressure refines subgroups with varying survival probabilities. This tree is simple enough for clinical use while providing actionable insights.

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.width=5, fig.height=3, fig.align='center'}
rpart.plot(tm2, main = "Gini, cp = 0.01", digits = 3)
```
-**Interpretation:**

- The tree starts with Consciousness Level < 1 as the primary split, indicating its importance in predicting survival.
- The next split is Blood Pressure >= 88. 

At the root node the proportion of patients who survive (class = 0) is 0.200 (20%), and the proportion who do not survive (class = 1) is 0.800 (80%). Moving to the right child node (Consciousness Level >= 1), the probability of not surviving (class = 1) is 0.876, meaning that the probability of surviving if the patient is not conscious is 13.3%.
In the left child node, the probability of surviving (class = 0) is 0.146, this means that among conscious patients, 14.6% survive and 85.4% do not.
Then among the conscious patients, if their blood pressure is greater than or equal to 88, the probability of surviving is 0.119 and the probability of not surviving is 0.881. This means that among conscious patients with blood pressure greater than or equal to 88, 11.9% survive and 88.1%. However if the blood pressure is less than 88, the probability of not surviving is 0.667 and the probability of surviving is 0.333. This means that among conscious patients with blood pressure less than 88, 33.3% survive and 66.7% do

\newpage

## Question 2

In this question we have to assess how well the chosen tree predicts for Survival, using the AUC metric, to measure predictive performance. Finally we will compare the AUC of the decision tree with the AUC of the logistic regression model.

### Approach:

- **Calculate Predicted Probabilities**:
  Use the `predict()` function to calculate the predicted probabilities for the chosen decision tree model.
- **Calculate AUC**:
  Use the `roc()` function from the `pROC` package to calculate the AUC for the decision tree model.
- **Logistic Regression Model**:
  Compare the AUC of the decision tree with the AUC of the logistic regression model to assess predictive performance.

### Results:

```{r echo=FALSE, message=FALSE, warning=FALSE}
library(pROC)
library(kableExtra)

# Predicted probabilities for the decision tree
pred_tm2 <- predict(tm2, type = "prob")[, 2]  # Probabilities of 'Not Survived'

# AUC for the decision tree
auc_tm2 <- auc(data_ca4$Survival, pred_tm2)

# Predicted probabilities for the logistic regression model
pred_logit <- predict(m_step, type = "response") 

# AUC for the logistic regression model
auc_logit <- auc(data_ca4$Survival, pred_logit)
# Create the comparison table
auc_comparison <- data.frame(
  Model = c("Decision Tree (Gini, cp = 0.01)", "Logistic Regression"),
  AUC = c(auc_tm2, auc_logit)
)

# Generate a formatted table
kable(auc_comparison) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), 
                full_width = FALSE, 
                position = "center") %>%
  row_spec(1, color = "black", background = "white") %>%
  row_spec(2, color = "black", background = "white")
```

1. **Decision Tree AUC**: The decision tree model with Gini Index and cp = 0.01 has an AUC of 0.8118, indicating a good predictive performance.

2. **Logistic Regression AUC**: The logistic regression model has an AUC of 0.8729, which is higher than the decision tree model.

### Interpretation:

1. **Decision Tree vs. Logistic Regression**:
The decision tree is slightly less accurate but offers better interpretation making it a useful option when simplicity is a priority. (For real-time clinical applications)

2. **Logistic Regression**: 
Logistic regression achieves higher predictive accuracy, which may be preferable if accuracy outweighs interpretation.

In conclusion based on the AUC values, the logistic regression model is the better-performing model for predicting survival. However, the decision tree remains as a valuable option for its interpretation and simplicity, especially in clinical settings where understanding the decision-making process is crucial.

## Question 3

Calculate a LOOCV-corrected AUC for the decision tree model and comment on the result.

### Approach:

- **LOOCV for Decision Tree**:

  Perform Leave One Out Cross Validation (LOOCV) for the decision tree model to calculate the LOOCV-adjusted AUC.
- We have chosen the above mentioned best decision tree model with Gini Index and cp = 0.01 for this analysis.

### Results:

```{r echo=FALSE, message=FALSE, warning=FALSE}
# Create a vector for the predicted values
predprob_tm_LOOCV <- numeric(nrow(data_ca4)) 

for (i in 1:nrow(data_ca4)) {
  # Create training and validation sets
  data_training_tree <- data_ca4[-i,]
  data_validation_tree <- data_ca4[i, ,drop=FALSE]
  # Fit the model on the training data
  tm_loocv <- rpart(Survival ~.,method = "class", data=data_training_tree, parms = list(split = "gini"), cp = 0.01)
  # Predict the value for the held-out observation
  predprob_tm_LOOCV[i] <- predict(tm_loocv, newdata = data_validation_tree, type = "prob")[,2]
}

roc(data_ca4$Survival, predprob_tm_LOOCV)
```


### Interpretation:

- The LOOCV-adjusted AUC is 0.6284 which is a significant decrease from the unadjusted AUC of 0.8115. This indicates that the decision tree model may be overfitting the data, leading to a lower predictive performance on unseen data. 
- The decision tree model may benefit from additional tuning or simplification to enhance its predictive performance and avoid overfitting.
- This could be due to the presence of the parameters BloodPressure and Patient ID in the model, which may not be significant predictors of survival and could be contributing to overfitting.